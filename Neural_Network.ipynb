{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2641f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize as sp\n",
    "import math\n",
    "import Set_Emnist_Data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aba160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates sigmoid function, params: z (variable), returns: sigmoid calculation\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "#calculates the sigmoidGradient (derivative), params: z (variable), returns: sigmoid gradient calculation \n",
    "def sigmoidGradient(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "#calculates the regularization term for the neural network, params: lambda (regularization constant),\n",
    "#m (number of training samples), returns regularization value\n",
    "def regularization(lamda, m):\n",
    "    lamda_val = lamda/(2.0*m)\n",
    "    theta1_sum = 0 \n",
    "    theta2_sum = 0\n",
    "    for j in range(len(data.theta1)-1):\n",
    "        for k in range(data.theta1[0].size-1):\n",
    "            theta1_sum += data.theta1[j+1][k+1]*data.theta1[j+1][k+1]\n",
    "    for j in range(len(data.theta2)-1):\n",
    "        for k in range(data.theta2[0].size-1):\n",
    "            theta2_sum += data.theta2[j+1][k+1]*data.theta2[j+1][k+1]\n",
    "    return lamda_val*(theta1_sum+theta2_sum)\n",
    "\n",
    "#calculates the cost for the neural network, params: y_vals (expected output values), hyp (calculated output values),\n",
    "#m (number of training samples), returns cost between given sample and expected value  \n",
    "def calc_cost(y_vals, hyp, lamda, m): #hyp and y are both 10x1 vectors \n",
    "    cost = 0\n",
    "    for k in range(y_vals.size):\n",
    "        cost += -y_vals[k] * math.log(hyp[k]) - (1-y_vals[k])*math.log(1-hyp[k])\n",
    "    return cost\n",
    "\n",
    "#predicts the number that correlates to the input data, params: weights(an array that consists of 2 weight matricies),\n",
    "#x_vals (array that consists of input values), returns prediction number (0-9) \n",
    "def predict(weights, x_vals):\n",
    "        #x_vals = np.hstack(([1],x_vals))\n",
    "        weights1 = weights[0]\n",
    "        weights2 = weights[1]\n",
    "        z2 = np.matmul(x_vals,weights1.T)\n",
    "        a2 = sigmoid(z2)\n",
    "        a2 = np.hstack(([1], a2))\n",
    "        z3 = np.matmul(a2,weights2.T)\n",
    "        a3 = sigmoid(z3)\n",
    "        max_val = a3[0]\n",
    "        max_index = 0\n",
    "        print(a3)\n",
    "        for i in range(len(a3)):\n",
    "            if (a3[i] > max_val):\n",
    "                max_val = a3[i]\n",
    "                max_index = i\n",
    "        prediction = max_index+1\n",
    "        if prediction == 10:\n",
    "            prediction = 0\n",
    "        return prediction\n",
    "\n",
    "#performs forward and backward prop to get a final cost value, J, and 2 gradient weight matricies\n",
    "#params: nn_params(array that consists of 2 weight matricies for layer 1 and 2 respectively), input_layer_size (number of input units),\n",
    "#hidden_layer_size (number of hidden units), num_labels (number of output units), x (training samples), y (expected output values), lambda_reg (regularization constant)\n",
    "#returns cost and an array of weight gradient vectors \n",
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_reg):\n",
    "    data.theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)],(hidden_layer_size, input_layer_size+1))\n",
    "    data.theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "\n",
    "    J = 0;\n",
    "    Theta1_grad = np.zeros_like(data.theta1)\n",
    "    Theta2_grad = np.zeros_like(data.theta2)\n",
    "\n",
    "    #Forward and Back prop: \n",
    "\n",
    "    bigDelta1 = 0\n",
    "    bigDelta2 = 0\n",
    "    cost_temp = 0\n",
    "\n",
    "    # for each training example\n",
    "    for t in range(data.m):\n",
    "\n",
    "        ## step 1: perform forward pass\n",
    "        x = X[t]\n",
    "\n",
    "        #calculate z2 (linear combination) and a2 (activation for layer 2)\n",
    "        z2 = np.matmul(x,data.theta1.T)\n",
    "        a2 = sigmoid(z2)\n",
    "\n",
    "        # add column of ones as bias unit to the second layer\n",
    "        a2 = np.hstack(([1], a2))\n",
    "        # calculate z3 (linear combination) and a3 (activation for layer 3 aka final hypothesis)\n",
    "        z3 = np.matmul(a2,data.theta2.T)\n",
    "        a3 = sigmoid(z3)\n",
    "\n",
    "        #Backpropogation: \n",
    "\n",
    "        #step 2: set delta 3\n",
    "        delta3 = np.zeros((num_labels))\n",
    "\n",
    "        #Get Error: subtract actual val in y from each hypothesized val in a3  \n",
    "        y_vals = np.zeros((num_labels))\n",
    "        for k in range(num_labels): #for each of the 10 labels subtract\n",
    "            y_k = y[t][k]\n",
    "            y_vals[k] = y_k\n",
    "            delta3[k] = a3[k] - y_k\n",
    "\n",
    "        #step 3: for layer 2 set delta2 = Theta2 Transpose * delta3 .* sigmoidGradient(z2) (= Chain Rule)\n",
    "        #Skip over the bias unit in layer 2: no gradient calculated for this value \n",
    "        delta2 = np.matmul(data.theta2[:,1:].T, delta3) * sigmoidGradient(z2)\n",
    "\n",
    "        #step 4: accumulate gradient from this sample\n",
    "        bigDelta1 += np.outer(delta2, x)\n",
    "        bigDelta2 += np.outer(delta3, a2)\n",
    "        #Update the total cost given the cost from this sample\n",
    "        cost_temp += calc_cost(y_vals, a3, lambda_reg, data.m)\n",
    "\n",
    "    #Accumulate cost values and regularize to get Cost(J) \n",
    "    term1 = (1/data.m)*cost_temp\n",
    "    term2 = regularization(lambda_reg, data.m)\n",
    "    J = term1 + term2\n",
    "\n",
    "    # step 5: obtain gradient for neural net cost function by dividing the accumulated gradients by m\n",
    "    Theta1_grad = bigDelta1 / data.m\n",
    "    Theta2_grad = bigDelta2 / data.m\n",
    "\n",
    "\n",
    "    #Regularization\n",
    "    #only regularize for j >= 1, so skip the first column\n",
    "    Theta1_grad_unregularized = np.copy(Theta1_grad)\n",
    "    Theta2_grad_unregularized = np.copy(Theta2_grad)\n",
    "    Theta1_grad += (float(lambda_reg)/data.m)*data.theta1\n",
    "    Theta2_grad += (float(lambda_reg)/data.m)*data.theta2\n",
    "    Theta1_grad[:,0] = Theta1_grad_unregularized[:,0]\n",
    "    Theta2_grad[:,0] = Theta2_grad_unregularized[:,0]\n",
    "    flattened_grads = np.hstack((Theta1_grad.flatten(),Theta2_grad.flatten()))\n",
    "\n",
    "    return J, flattened_grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
