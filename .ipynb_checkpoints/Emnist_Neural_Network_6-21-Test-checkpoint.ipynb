{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fb3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize as sp\n",
    "import pandas as pd\n",
    "import math\n",
    "from random import sample, uniform\n",
    "\n",
    "class Neural_Network():\n",
    "    #calculates sigmoid function, params: z (variable), returns: sigmoid calculation\n",
    "    def sigmoid(self,z):\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    #calculates the sigmoidGradient (derivative), params: z (variable), returns: sigmoid gradient calculation \n",
    "    def sigmoidGradient(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "    \n",
    "    def randomly_initialize(self, num_in, num_out):\n",
    "        epsilon = 0.12\n",
    "        size = num_out*(num_in+1)\n",
    "        all_weights = np.zeros((size,))\n",
    "        for i in range(size):   \n",
    "            all_weights[i] = uniform(-epsilon, epsilon) \n",
    "        weights = np.reshape(all_weights,(num_out,num_in+1))\n",
    "        return weights\n",
    "    \n",
    "    #calculates the regularization term for the neural network, params: lambda (regularization constant),\n",
    "    #m (number of training samples), returns regularization value\n",
    "    def regularization(self, lamda, m):\n",
    "        lamda_val = lamda/(2.0*m)\n",
    "        theta1_sum = 0 \n",
    "        theta2_sum = 0\n",
    "        for j in range(len(self.Theta1)-1):\n",
    "            for k in range(self.Theta1[0].size-1):\n",
    "                theta1_sum += self.Theta1[j+1][k+1]*self.Theta1[j+1][k+1]\n",
    "        for j in range(len(self.Theta2)-1):\n",
    "            for k in range(self.Theta2[0].size-1):\n",
    "                theta2_sum += self.Theta2[j+1][k+1]*self.Theta2[j+1][k+1]\n",
    "        return lamda_val*(theta1_sum+theta2_sum)\n",
    "    \n",
    "    #calculates the cost for the neural network, params: y_vals (expected output values), hyp (calculated output values),\n",
    "    #m (number of training samples), returns cost between given sample and expected value  \n",
    "    def calc_cost(self, y_vals, hyp, lamda, m): #hyp and y are both 10x1 vectors \n",
    "        cost = 0\n",
    "        for k in range(len(y_vals)):\n",
    "            cost += -y_vals[k] * math.log(hyp[k]) - (1-y_vals[k])*math.log(1-hyp[k])\n",
    "        return cost\n",
    "    \n",
    "    #predicts the number that correlates to the input data, params: weights(an array that consists of 2 weight matricies),\n",
    "    #x_vals (array that consists of input values), returns prediction number (0-9) \n",
    "    def predict(self, weights, x_vals):\n",
    "            #x_vals = np.hstack(([1],x_vals))\n",
    "            weights1 = weights[0]\n",
    "            weights2 = weights[1]\n",
    "            z2 = np.matmul(x_vals,weights1.T)\n",
    "            a2 = self.sigmoid(z2)\n",
    "            a2 = np.hstack(([1], a2))\n",
    "            z3 = np.matmul(a2,weights2.T)\n",
    "            a3 = self.sigmoid(z3)\n",
    "            max_val = a3[0]\n",
    "            max_index = 0\n",
    "            print(a3)\n",
    "            for i in range(len(a3)):\n",
    "                if (a3[i] > max_val):\n",
    "                    max_val = a3[i]\n",
    "                    max_index = i\n",
    "            prediction = max_index+1\n",
    "            if prediction == 10:\n",
    "                prediction = 0\n",
    "            return prediction\n",
    "        \n",
    "    #performs forward and backward prop to get a final cost value, J, and 2 gradient weight matricies\n",
    "    #params: nn_params(array that consists of 2 weight matricies for layer 1 and 2 respectively), input_layer_size (number of input units),\n",
    "    #hidden_layer_size (number of hidden units), num_labels (number of output units), x (training samples), y (expected output values), lambda_reg (regularization constant)\n",
    "    #returns cost and an array of weight gradient vectors \n",
    "    def nnCostFunction(self, nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_reg):\n",
    "        self.Theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)],(hidden_layer_size, input_layer_size+1))\n",
    "        self.Theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "        \n",
    "        J = 0;\n",
    "        Theta1_grad = np.zeros_like(self.Theta1)\n",
    "        Theta2_grad = np.zeros_like(self.Theta2)\n",
    "       \n",
    "        #Forward and Back prop: \n",
    "\n",
    "        bigDelta1 = 0\n",
    "        bigDelta2 = 0\n",
    "        cost_temp = 0\n",
    "\n",
    "        # for each training example\n",
    "        for t in range(m):\n",
    "\n",
    "            ## step 1: perform forward pass\n",
    "            x = X[t]\n",
    "\n",
    "            #calculate z2 (linear combination) and a2 (activation for layer 2)\n",
    "            z2 = np.matmul(x,self.Theta1.T)\n",
    "            a2 = self.sigmoid(z2)\n",
    "\n",
    "            # add column of ones as bias unit to the second layer\n",
    "            a2 = np.hstack(([1], a2))\n",
    "            # calculate z3 (linear combination) and a3 (activation for layer 3 aka final hypothesis)\n",
    "            z3 = np.matmul(a2,self.Theta2.T)\n",
    "            a3 = self.sigmoid(z3)\n",
    "            \n",
    "            #Backpropogation: \n",
    "\n",
    "            #step 2: set delta 3\n",
    "            delta3 = np.zeros((num_labels))\n",
    "\n",
    "            #Get Error: subtract actual val in y from each hypothesized val in a3  \n",
    "            y_vals = np.zeros((num_labels))\n",
    "            for k in range(num_labels): #for each of the 10 labels subtract\n",
    "                y_k = y[t][k]\n",
    "                y_vals[k] = y_k\n",
    "                delta3[k] = a3[k] - y_k\n",
    "\n",
    "            #step 3: for layer 2 set delta2 = Theta2 Transpose * delta3 .* sigmoidGradient(z2) (= Chain Rule)\n",
    "            #Skip over the bias unit in layer 2: no gradient calculated for this value \n",
    "            delta2 = np.matmul(self.Theta2[:,1:].T, delta3) * self.sigmoidGradient(z2)\n",
    "\n",
    "            #step 4: accumulate gradient from this sample\n",
    "            bigDelta1 += np.outer(delta2, x)\n",
    "            bigDelta2 += np.outer(delta3, a2)\n",
    "            #Update the total cost given the cost from this sample\n",
    "            cost_temp += self.calc_cost(y_vals, a3, lambda_reg, m)\n",
    "            \n",
    "        #Accumulate cost values and regularize to get Cost(J) \n",
    "        term1 = (1/m)*cost_temp\n",
    "        term2 = self.regularization(lambda_reg, m)\n",
    "        J = term1 + term2\n",
    "        print(\"Cost: \" + str(J)) \n",
    "        \n",
    "        # step 5: obtain gradient for neural net cost function by dividing the accumulated gradients by m\n",
    "        Theta1_grad = bigDelta1 / m\n",
    "        Theta2_grad = bigDelta2 / m\n",
    "        \n",
    "\n",
    "        #Regularization\n",
    "        #only regularize for j >= 1, so skip the first column\n",
    "        Theta1_grad_unregularized = np.copy(Theta1_grad)\n",
    "        Theta2_grad_unregularized = np.copy(Theta2_grad)\n",
    "        Theta1_grad += (float(lambda_reg)/m)*self.Theta1\n",
    "        Theta2_grad += (float(lambda_reg)/m)*self.Theta2\n",
    "        Theta1_grad[:,0] = Theta1_grad_unregularized[:,0]\n",
    "        Theta2_grad[:,0] = Theta2_grad_unregularized[:,0]\n",
    "        flattened_grads = np.hstack((Theta1_grad.flatten(),Theta2_grad.flatten()))\n",
    "        \n",
    "        return J, flattened_grads\n",
    "        \n",
    "        \n",
    "#Read in data files\n",
    "#df = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Theta1.csv', header = None)\n",
    "#df2 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Theta2.csv', header = None)\n",
    "df3 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/X.csv', header = None)\n",
    "df4 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Y.csv', header = None)\n",
    "#Initialize layer sizes\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 100\n",
    "num_labels = 10\n",
    "#Set sizes for weight and data matricies\n",
    "theta1 = np.zeros([hidden_layer_size,input_layer_size+1])\n",
    "theta2 = np.zeros([num_labels,hidden_layer_size+1])\n",
    "x = np.zeros((len(df3),input_layer_size))\n",
    "x_sample = np.zeros((int(len(df3)*(0.7)), input_layer_size)) #take only 70% for training to leave 30% for testing\n",
    "y_vec = np.zeros((len(df4),))\n",
    "y_sample = np.zeros((int(len(df4)*0.7)),)\n",
    "random_indicies = sample(range(0,int(len(df3))),int(len(df3)*0.7)) \n",
    "\n",
    "n = Neural_Network()\n",
    "\n",
    "#create data and weight arrays\n",
    "theta1 = n.randomly_initialize(input_layer_size, hidden_layer_size)\n",
    "theta2 = n.randomly_initialize(hidden_layer_size, num_labels)\n",
    "\n",
    "index = 0\n",
    "while(index<len(x)):\n",
    "    x[index] = df3.iloc[index]\n",
    "    index+=1\n",
    "\n",
    "index = 0\n",
    "while (index<len(y_vec)):\n",
    "    y_vec[index] = df4.iloc[index]\n",
    "    index+=1\n",
    "    \n",
    "for index in range(len(random_indicies)):\n",
    "    sample_index = random_indicies[index] \n",
    "    x_sample[index] = x[sample_index] \n",
    "    y_sample[index] = y_vec[sample_index]\n",
    "x_test = np.zeros((int(len(df3)*0.3),input_layer_size))\n",
    "y_test = np.zeros((int(len(df4)*0.3),))\n",
    "#set test data\n",
    "test_indicies = np.zeros((int(len(df3)*0.3),))\n",
    "count = 0\n",
    "for ind in range(len(df3)):\n",
    "    if ind not in random_indicies:\n",
    "        test_indicies[count] = ind\n",
    "        count+=1\n",
    "for ii in range(len(test_indicies)):\n",
    "    test_index = int(test_indicies[ii])\n",
    "    x_test[ii] = x[test_index]\n",
    "    y_test[ii] = y_vec[test_index]\n",
    "    \n",
    "x = x_sample\n",
    "ones = np.ones((len(x_sample),1))\n",
    "test_ones = np.ones((len(x_test),1))\n",
    "x = np.hstack((ones, x)) \n",
    "x_test = np.hstack((test_ones,x_test))\n",
    "y_vec = y_sample\n",
    "    \n",
    "m = len(x)\n",
    "# set y to be a 2-D matrix with each column being a different sample and each row corresponding to a value 0-9\n",
    "y = np.zeros((m,num_labels))\n",
    "y_test_matrix = np.zeros((len(y_test),num_labels))\n",
    "# for every label, convert it into vector of 0s and a 1 in the appropriate position\n",
    "for i in range(m): #each row is new training sample\n",
    "    index = int(y_vec[i]-1)\n",
    "    y[i][index] = 1\n",
    "y_temp = y_test\n",
    "for j in range(int(len(y_test))):\n",
    "    index2 = int(y_temp[j]-1)\n",
    "    y_test_matrix[j][index2] = 1\n",
    "y_test = y_test_matrix\n",
    "nn_params = [theta1, theta2] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5063818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.45553961417519007\n",
      "Cost: 0.45553961417519007\n",
      "Cost: 0.45553961416554456\n",
      "Cost: 0.4555396141648935\n",
      "Cost: 0.4555396141489515\n",
      "Cost: 0.4555396141221665\n",
      "Cost: 0.4555396141284537\n",
      "Cost: 0.45553961409115235\n",
      "Cost: 0.4555396139778548\n",
      "Cost: 0.455539614121926\n",
      "Cost: 0.45553961411810384\n",
      "Cost: 0.4555396140711747\n",
      "Cost: 0.45553961403088417\n",
      "Cost: 0.45553961413330407\n",
      "Cost: 0.45553961407296484\n",
      "Cost: 0.45553961412470373\n",
      "Cost: 0.4554714282999893\n",
      "Cost: 0.4554714282778789\n",
      "Cost: 0.45547142827915843\n",
      "Cost: 0.45547142828606535\n",
      "Cost: 0.4554714282734295\n",
      "Cost: 0.45547142827635456\n",
      "Cost: 0.45547142828028137\n",
      "Cost: 0.4554714282621329\n",
      "Cost: 0.45547142826841014\n",
      "Cost: 0.45547142828449316\n",
      "Cost: 0.4554714282738146\n",
      "Cost: 0.4554714282735708\n",
      "Cost: 0.4554714282834794\n",
      "Cost: 0.45547142827430276\n",
      "Cost: 0.4554714282810818\n",
      "Cost: 0.45547142828414877\n",
      "Cost: 0.4554714282838664\n",
      "Cost: 0.45547142828587894\n",
      "Cost: 0.45547142829278076\n",
      "Cost: 0.45547142829381954\n",
      "Cost: 0.45536610588963555\n",
      "Cost: 0.4553661058542021\n",
      "Cost: 0.45536610588457604\n",
      "Cost: 0.45536610588111315\n",
      "Cost: 0.45536610588298543\n",
      "Cost: 0.45536610587916204\n",
      "Cost: 0.4553661058813461\n",
      "Cost: 0.45536610588276955\n",
      "Cost: 0.45536610587525794\n",
      "Cost: 0.45536610588002496\n",
      "Cost: 0.45536610587960336\n",
      "Cost: 0.45536610587587606\n",
      "Cost: 0.45536610588242615\n",
      "Cost: 0.4553661058763041\n",
      "Cost: 0.4553661058843387\n",
      "Cost: 0.4553661058805717\n",
      "Cost: 0.45536610588499316\n",
      "Cost: 0.45536610588513005\n",
      "Cost: 0.45536610588679516\n",
      "Cost: 0.4553661058886459\n",
      "Cost: 0.4553290915058438\n",
      "Cost: 0.45532909147242223\n",
      "Cost: 0.45532909150365425\n",
      "Cost: 0.4553290915019763\n",
      "Cost: 0.4553249058312363\n",
      "Cost: 0.45532490582700025\n",
      "Cost: 0.45532490582834734\n",
      "Cost: 0.45532490582385904\n",
      "Cost: 0.45532490582747853\n",
      "Cost: 0.4553249058224917\n",
      "Cost: 0.4553249058256357\n",
      "Cost: 0.45532490582429713\n",
      "Cost: 0.45532490582288443\n",
      "Cost: 0.4553249058255684\n",
      "Cost: 0.45532490582725166\n",
      "Cost: 0.45532490582390794\n",
      "Cost: 0.4553249058256338\n",
      "Cost: 0.45532490582767005\n",
      "Cost: 0.4553249058281389\n",
      "Cost: 0.4553249058265008\n",
      "Cost: 0.4553249058286216\n",
      "Cost: 0.45530445165583006\n",
      "Cost: 0.4553044516514278\n",
      "Cost: 0.45530445165326056\n",
      "Cost: 0.4553044516515913\n",
      "Cost: 0.4553044516526126\n",
      "Cost: 0.4553044516501563\n",
      "Cost: 0.45530445165096195\n",
      "Cost: 0.45530445165113764\n",
      "Cost: 0.45530445164281236\n",
      "Cost: 0.4553044516502955\n",
      "Cost: 0.45530445164111666\n",
      "Cost: 0.45530445164861677\n",
      "Cost: 0.455304451642913\n",
      "Cost: 0.45530445164929745\n",
      "Cost: 0.4553044516467114\n",
      "Cost: 0.4553044516515866\n",
      "Cost: 0.455304451650147\n",
      "Cost: 0.45530445165071376\n",
      "Cost: 0.45530445164772004\n",
      "Cost: 0.45530445164575656\n",
      "Cost: 0.4553044516478122\n",
      "Cost: 0.45530445163295485\n",
      "Cost: 0.45530445165129463\n",
      "Cost: 0.45530445164238065\n",
      "Cost: 0.45523828889616264\n",
      "Cost: 0.4552382888899962\n",
      "Cost: 0.45523828888755347\n",
      "Cost: 0.4552382888913633\n",
      "Cost: 0.4552382888911393\n",
      "Cost: 0.4552382888872507\n",
      "Cost: 0.4552382888879804\n",
      "Cost: 0.4552382888891846\n",
      "Cost: 0.4552382888816362\n",
      "Cost: 0.4552382888895219\n",
      "Cost: 0.45523828888779255\n",
      "Cost: 0.4552382888914581\n",
      "Cost: 0.45523828887771756\n",
      "Cost: 0.45523828888583995\n",
      "Cost: 0.45523828888159745\n",
      "Cost: 0.4552382888756924\n",
      "Cost: 0.4552382888849133\n",
      "Cost: 0.45523828887887924\n",
      "Cost: 0.4552382888856864\n",
      "Cost: 0.4552382888856603\n",
      "Cost: 0.45523828888758344\n",
      "Cost: 0.4552382888814841\n",
      "Cost: 0.45519057930892526\n",
      "Cost: 0.45518268751638674\n",
      "Cost: 0.455175610052636\n",
      "Cost: 0.45517561004361073\n",
      "Cost: 0.4551756100466274\n",
      "Cost: 0.4551756100441442\n",
      "Cost: 0.4551756100454969\n",
      "Cost: 0.45517561004628027\n",
      "Cost: 0.45517561004646\n",
      "Cost: 0.4551756100466628\n",
      "Cost: 0.4551756100447307\n",
      "Cost: 0.4551756100504843\n",
      "Cost: 0.45517561004844964\n",
      "Cost: 0.45517561005046125\n",
      "Cost: 0.45517561004719237\n",
      "Cost: 0.45517561004659035\n",
      "Cost: 0.45517561004716667\n",
      "Cost: 0.45517561004534196\n",
      "Cost: 0.45517561004842205\n",
      "Cost: 0.4551756100479124\n",
      "Cost: 0.45517561004543383\n",
      "Cost: 0.45517561004916096\n",
      "Cost: 0.4551756100494267\n",
      "Cost: 0.45517561004704\n",
      "Cost: 0.4551505490980248\n",
      "Cost: 0.4551505490945026\n",
      "Cost: 0.45515054909486097\n",
      "Cost: 0.45515054909676045\n",
      "Cost: 0.4551505490940162\n",
      "Cost: 0.4551505490957396\n",
      "Cost: 0.45515054909558816\n",
      "Cost: 0.4551505490937031\n",
      "Cost: 0.4551505490950296\n",
      "Cost: 0.45515054909413355\n",
      "Cost: 0.45515054909171826\n",
      "Cost: 0.45515054909423214\n",
      "Cost: 0.45515054908758457\n",
      "Cost: 0.45515054909359526\n",
      "Cost: 0.4551505490908525\n",
      "Cost: 0.4551505490949547\n",
      "Cost: 0.4551505490946166\n",
      "Cost: 0.45515054909290686\n",
      "Cost: 0.45515054909452923\n",
      "Cost: 0.45515054909526415\n",
      "Cost: 0.4551505490952681\n",
      "Cost: 0.4551505490934366\n",
      "Cost: 0.4551505490824592\n",
      "Cost: 0.45515054909263586\n",
      "Cost: 0.45515054909383623\n",
      "Cost: 0.4551222676666369\n",
      "Cost: 0.45512226766147323\n",
      "Cost: 0.4551222676638008\n",
      "Cost: 0.45512226766402913\n",
      "Cost: 0.45512226766423913\n",
      "Cost: 0.45512226766272446\n",
      "Cost: 0.45512226766504593\n",
      "Cost: 0.45512226766466796\n",
      "Cost: 0.45512226766438685\n",
      "Cost: 0.4551222676645016\n",
      "Cost: 0.4551222676637089\n",
      "Cost: 0.45512226766474506\n",
      "Cost: 0.45512226766358177\n",
      "Cost: 0.45512226766488284\n",
      "Cost: 0.45512226766298214\n",
      "Cost: 0.4551222676640989\n",
      "Cost: 0.4551222676616117\n",
      "Cost: 0.45512226766163044\n",
      "Cost: 0.45512226766292185\n",
      "Cost: 0.4551222676647071\n",
      "Cost: 0.4551222676647685\n",
      "Cost: 0.45512226766457253\n",
      "Cost: 0.45510665992345634\n",
      "Cost: 0.455106659921292\n",
      "Cost: 0.45510665992102256\n",
      "Cost: 0.45510665992236493\n",
      "Cost: 0.4551066599213228\n",
      "Cost: 0.4551066599213075\n",
      "Cost: 0.4551066599215956\n",
      "Cost: 0.45510665992072097\n",
      "Cost: 0.45510665992061705\n",
      "Cost: 0.4551066599196697\n",
      "Cost: 0.45510665991956056\n",
      "Cost: 0.45510665992102634\n",
      "Cost: 0.45510665991748706\n",
      "Cost: 0.4551066599208748\n",
      "Cost: 0.45510665991918164\n",
      "Cost: 0.4551066599204772\n",
      "Cost: 0.4551066599212782\n",
      "Cost: 0.4551066599213246\n",
      "Cost: 0.455106659921676\n",
      "Cost: 0.45510665992248966\n",
      "Cost: 0.45510665992277355\n",
      "Cost: 0.4551066599227415\n",
      "Cost: 0.45509469324189455\n",
      "Cost: 0.4550946932373954\n",
      "Cost: 0.45509469324031493\n",
      "Cost: 0.4550946932400257\n",
      "Cost: 0.4550946932406193\n",
      "Cost: 0.4550946932398948\n",
      "Cost: 0.4550946932408617\n",
      "Cost: 0.45509469324021057\n",
      "Cost: 0.4550946932398545\n",
      "Cost: 0.4550946932403287\n",
      "Cost: 0.45509469323964236\n",
      "Cost: 0.45509469324022334\n",
      "Cost: 0.4550946932376045\n",
      "Cost: 0.45509469323933727\n",
      "Cost: 0.45509469323836926\n",
      "Cost: 0.45509469323914065\n",
      "Cost: 0.45509469323836726\n",
      "Cost: 0.4550946932368562\n",
      "Cost: 0.4550946932391061\n",
      "Cost: 0.4550946932405901\n",
      "Cost: 0.45509469324108737\n",
      "Cost: 0.4550946932402947\n",
      "Cost: 0.4550946932388748\n",
      "Cost: 0.45509469324029556\n",
      "Cost: 0.45509469324042895\n",
      "Cost: 0.45509469323649576\n",
      "Cost: 0.4550946932399015\n",
      "Cost: 0.4550946932392347\n",
      "Cost: 0.45507478523423806\n",
      "Cost: 0.4550747852316439\n",
      "Cost: 0.45507478523223344\n",
      "Cost: 0.45507478523190337\n",
      "Cost: 0.4550747852325905\n",
      "Cost: 0.45507478523094835\n",
      "Cost: 0.4550747852319746\n",
      "Cost: 0.45507478523204203\n",
      "Cost: 0.45507478523148404\n",
      "Cost: 0.4550747852324689\n",
      "Cost: 0.45507478523116884\n",
      "Cost: 0.45507478523280104\n",
      "Cost: 0.45507478523082545\n",
      "Cost: 0.45507478523200245\n",
      "Cost: 0.4550747852313539\n",
      "Cost: 0.4550747852299596\n",
      "Cost: 0.4550747852310707\n",
      "Cost: 0.4550747852288758\n",
      "Cost: 0.45507478523073974\n",
      "Cost: 0.45507478523122025\n",
      "Cost: 0.455074785232392\n",
      "Cost: 0.4550747852324931\n",
      "Cost: 0.45507478522704237\n",
      "Cost: 0.45507478523166167\n",
      "Cost: 0.45507478523277145\n",
      "Cost: 0.45507478522838296\n",
      "Cost: 0.45507478523317\n",
      "Cost: 0.4550747852304571\n",
      "Cost: 0.45507478523297445\n",
      "Cost: 0.4550747852303999\n",
      "Cost: 0.45507478523270295\n",
      "Cost: 0.45507478522972067\n",
      "Cost: 0.45507478523322425\n",
      "Cost: 0.4550747852310274\n",
      "Cost: 0.455074785232408\n",
      "Cost: 0.45507478523207734\n",
      "Cost: 0.45507478523216716\n",
      "Cost: 0.45507478523256295\n",
      "Cost: 0.4550747852330441\n",
      "Cost: 0.45507478523109524\n",
      "Cost: 0.45507478523296563\n",
      "Cost: 0.4550258563003451\n",
      "Cost: 0.4550258562976043\n",
      "Cost: 0.4550258562994499\n",
      "Cost: 0.45502585629905135\n",
      "Cost: 0.45502585629861764\n",
      "Cost: 0.4550258562994165\n",
      "Cost: 0.4550258562993884\n",
      "Cost: 0.455025856300016\n",
      "Cost: 0.4550258562993761\n",
      "Cost: 0.45502585629970554\n",
      "Cost: 0.45502585629956477\n",
      "Cost: 0.45502585629974146\n",
      "Cost: 0.4550258562998867\n",
      "Cost: 0.4550258562995573\n",
      "Cost: 0.45502585629986475\n",
      "Cost: 0.4550258562994073\n",
      "Cost: 0.45502585629968817\n",
      "Cost: 0.4550258562995767\n",
      "Cost: 0.45502585629971626\n",
      "Cost: 0.4550258562998992\n",
      "Cost: 0.45502585629988646\n",
      "Cost: 0.4550258562997027\n",
      "Cost: 0.45502585629904124\n",
      "Cost: 0.4550258562993909\n",
      "Cost: 0.4550258562984746\n",
      "Cost: 0.45502585629978587\n",
      "Cost: 0.4550258562986721\n",
      "Cost: 0.45502585629946135\n",
      "Cost: 0.45502585629979686\n",
      "Cost: 0.4550258562994557\n",
      "Cost: 0.4550258562996736\n",
      "Cost: 0.45502585629938735\n",
      "Cost: 0.45502585630027814\n",
      "Cost: 0.45502017128240335\n",
      "Cost: 0.45502017128124084\n",
      "Cost: 0.4550201712821157\n",
      "Cost: 0.45502017128178596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.45502017128157696\n",
      "Cost: 0.45502017128191524\n",
      "Cost: 0.4550201712818064\n",
      "Cost: 0.45502017128207833\n",
      "Cost: 0.45502017128134975\n",
      "Cost: 0.45502017128188355\n",
      "Cost: 0.4550201712812312\n",
      "Cost: 0.45502017128170563\n",
      "Cost: 0.45502017128097283\n",
      "Cost: 0.4550201712799783\n",
      "Cost: 0.4550201712810027\n",
      "Cost: 0.45502017128037453\n",
      "Cost: 0.4550201712815503\n",
      "Cost: 0.4550201712798335\n",
      "Cost: 0.45502017128125427\n",
      "Cost: 0.4550201712809633\n",
      "Cost: 0.45502017128098804\n",
      "Cost: 0.45502017128102856\n",
      "Cost: 0.4550201712809552\n",
      "Cost: 0.45502017127975164\n",
      "Cost: 0.455020171281201\n",
      "Cost: 0.45502017127935435\n",
      "Cost: 0.4550201712815673\n",
      "Cost: 0.455020171279977\n",
      "Cost: 0.45502017128126226\n",
      "Cost: 0.45502017128021943\n",
      "Cost: 0.4549995121087665\n",
      "Cost: 0.45499951210790646\n",
      "Cost: 0.4549995121083329\n",
      "Cost: 0.45499951210818185\n",
      "Cost: 0.4549995121083651\n",
      "Cost: 0.45499951210804856\n",
      "Cost: 0.45499951210819506\n",
      "Cost: 0.45499951210851614\n",
      "Cost: 0.4549995121079441\n",
      "Cost: 0.45499951210830825\n",
      "Cost: 0.45499951210846806\n",
      "Cost: 0.45499951210819206\n",
      "Cost: 0.45499951210800815\n",
      "Cost: 0.45499951210754674\n",
      "Cost: 0.45499951210759293\n",
      "Cost: 0.4549995121067715\n",
      "Cost: 0.4549995121077744\n",
      "Cost: 0.4549995121076653\n",
      "Cost: 0.45499951210616074\n",
      "Cost: 0.45499951210786926\n",
      "Cost: 0.45499951210788014\n",
      "Cost: 0.4549995121079535\n",
      "Cost: 0.45499951210744555\n",
      "Cost: 0.45499951210712913\n",
      "Cost: 0.45499951210808287\n",
      "Cost: 0.4549995121060889\n",
      "Cost: 0.45499951210810285\n",
      "Cost: 0.45499951210718625\n",
      "Cost: 0.45499951210783796\n",
      "Cost: 0.45499951210808076\n",
      "Cost: 0.45498727047997944\n",
      "Cost: 0.45498727047861875\n",
      "Cost: 0.4549872704792721\n",
      "Cost: 0.4549872704791791\n",
      "Cost: 0.454987270479332\n",
      "Cost: 0.4549872704789636\n",
      "Cost: 0.4549872704791783\n",
      "Cost: 0.45498727047903764\n",
      "Cost: 0.45498727047943166\n",
      "Cost: 0.45498727047886844\n",
      "Cost: 0.45498727047971776\n",
      "Cost: 0.45498727047953214\n",
      "Cost: 0.4549872704789972\n",
      "Cost: 0.45498727047901744\n",
      "Cost: 0.45498727047951104\n",
      "Cost: 0.45498727047933024\n",
      "Cost: 0.4549872704793361\n",
      "Cost: 0.4549872704794728\n",
      "Cost: 0.4549872704789457\n",
      "Cost: 0.45498727047944987\n",
      "Cost: 0.45498727047948545\n",
      "Cost: 0.4549872704793261\n",
      "Cost: 0.4549872704795215\n",
      "Cost: 0.45498577370768856\n",
      "Cost: 0.4549857737062003\n",
      "Cost: 0.45498577370715393\n",
      "Cost: 0.4549857737072619\n",
      "Cost: 0.45498577370689564\n",
      "Cost: 0.4549857737069978\n",
      "Cost: 0.4549857737072506\n",
      "Cost: 0.45498577370651705\n",
      "Cost: 0.4549857737070011\n",
      "Cost: 0.45498577370667015\n",
      "Cost: 0.4549857737066566\n",
      "Cost: 0.4549857737070836\n",
      "Cost: 0.45498577370667403\n",
      "Cost: 0.45498577370644816\n",
      "Cost: 0.4549857737071219\n",
      "Cost: 0.45498577370673354\n",
      "Cost: 0.4549857737073496\n",
      "Cost: 0.4549857737075097\n",
      "Cost: 0.4549857737074538\n",
      "Cost: 0.45498577370754933\n",
      "Cost: 0.45498577370749205\n",
      "Cost: 0.4549857737074692\n",
      "Cost: 0.4549857737074746\n",
      "Cost: 0.45498399897757047\n",
      "Cost: 0.4549839989767001\n",
      "Cost: 0.45498399897674535\n",
      "Cost: 0.4549839989770429\n",
      "Cost: 0.45498399897652997\n",
      "Cost: 0.45498399897665787\n",
      "Cost: 0.45498399897676906\n",
      "Cost: 0.45498399897616837\n",
      "Cost: 0.45498399897689723\n",
      "Cost: 0.45498399897573755\n",
      "Cost: 0.4549839989767026\n",
      "Cost: 0.4549839989758064\n",
      "Cost: 0.4549839989754799\n",
      "Cost: 0.4549839989752894\n",
      "Cost: 0.45498399897558467\n",
      "Cost: 0.454983998975486\n",
      "Cost: 0.4549839989745285\n",
      "Cost: 0.4549839989765315\n",
      "Cost: 0.45498399897576736\n",
      "Cost: 0.4549839989768278\n",
      "Cost: 0.45498399897675357\n",
      "Cost: 0.45498399897679165\n",
      "Cost: 0.4549839989768083\n",
      "Cost: 0.4549699725655612\n",
      "Cost: 0.4549699725643821\n",
      "Cost: 0.45496997256464\n",
      "Cost: 0.45496997256480975\n",
      "Cost: 0.45496997256445215\n",
      "Cost: 0.45496997256446925\n",
      "Cost: 0.4549699725645162\n",
      "Cost: 0.454969972563696\n",
      "Cost: 0.4549699725642648\n",
      "Cost: 0.45496997256354826\n",
      "Cost: 0.4549699725634837\n",
      "Cost: 0.4549699725640057\n",
      "Cost: 0.45496997256272487\n",
      "Cost: 0.4549699725632306\n",
      "Cost: 0.4549699725635456\n",
      "Cost: 0.45496997256341726\n",
      "Cost: 0.4549699725643638\n",
      "Cost: 0.4549699725641746\n",
      "Cost: 0.45496997256480753\n",
      "Cost: 0.45496997256495286\n",
      "Cost: 0.4549699725649672\n",
      "Cost: 0.4549699725649926\n",
      "Cost: 0.4549699725632947\n",
      "Cost: 0.4549699725644007\n",
      "Cost: 0.4549699725641428\n",
      "Cost: 0.4549699725622517\n",
      "Cost: 0.45496997256492666\n",
      "Cost: 0.45496997256406324\n",
      "Cost: 0.4549475452962205\n",
      "Cost: 0.45494754529486714\n",
      "Cost: 0.45494754529504144\n",
      "Cost: 0.4549475452954802\n",
      "Cost: 0.45494754529529985\n",
      "Cost: 0.4549475452944972\n",
      "Cost: 0.4549475452951779\n",
      "Cost: 0.4549475452941786\n",
      "Cost: 0.45494754529468606\n",
      "Cost: 0.4549475452947695\n",
      "Cost: 0.4549475452939307\n",
      "Cost: 0.45494754529503567\n",
      "Cost: 0.45494754529378695\n",
      "Cost: 0.4549475452933521\n",
      "Cost: 0.4549475452939892\n",
      "Cost: 0.4549475452934269\n",
      "Cost: 0.45494754529435266\n",
      "Cost: 0.4549475452928544\n",
      "Cost: 0.45494754529433185\n",
      "Cost: 0.45494754529497494\n",
      "Cost: 0.45494754529501374\n",
      "Cost: 0.45494754529512965\n",
      "Cost: 0.4549475452945182\n",
      "Cost: 0.454947545293652\n",
      "Cost: 0.4549475452953974\n",
      "Cost: 0.4549475452952045\n",
      "Cost: 0.4549163737414522\n",
      "Cost: 0.4549163737394204\n",
      "Cost: 0.4549163737402281\n",
      "Cost: 0.4549163737398712\n",
      "Cost: 0.45491637374017\n",
      "Cost: 0.45491637373928584\n",
      "Cost: 0.4549163737396512\n",
      "Cost: 0.454916373739528\n",
      "Cost: 0.45491637373827326\n",
      "Cost: 0.45491637373949795\n",
      "Cost: 0.4549163737368769\n",
      "Cost: 0.4549163737393782\n",
      "Cost: 0.4549163737373616\n",
      "Cost: 0.45491637373936356\n",
      "Cost: 0.45491637373753724\n",
      "Cost: 0.4549163737393387\n",
      "Cost: 0.45491637373719607\n",
      "Cost: 0.4549163737370507\n",
      "Cost: 0.45491637373920357\n",
      "Cost: 0.4549163737396553\n",
      "Cost: 0.45491637373953653\n",
      "Cost: 0.4549163737389651\n",
      "Cost: 0.45491637373706034\n",
      "Cost: 0.45491637373835103\n",
      "Cost: 0.45491637373986293\n",
      "Cost: 0.45491637373892946\n",
      "Cost: 0.45486795776516364\n",
      "Cost: 0.45488722712273455\n",
      "Cost: 0.45487592120430165\n",
      "Cost: 0.4548714431310723\n",
      "Cost: 0.4548695651835404\n",
      "Cost: 0.45486872618312607\n",
      "Cost: 0.4548683329623207\n",
      "Cost: 0.4548681430868782\n",
      "Cost: 0.45486804985379903\n",
      "Cost: 0.45486800366605074\n",
      "Cost: 0.4548679806797017\n",
      "Cost: 0.4548679692134502\n",
      "Cost: 0.45486796348706166\n",
      "Cost: 0.4548679606255507\n",
      "Cost: 0.4548679591952175\n",
      "Cost: 0.4548679584801567\n",
      "Cost: 0.45486795812265146\n",
      "Cost: 0.4548679579439059\n",
      "Cost: 0.454867957854534\n",
      "Cost: 0.45486795780984923\n",
      "Cost: 0.45486795778750655\n",
      "Cost: 0.45486795777633504\n",
      "Cost: 0.4548679577707504\n",
      "Cost: 0.45486795776246136\n",
      "Cost: 0.4548679577641827\n",
      "Cost: 0.4548679577652464\n",
      "Cost: 0.4548679577655482\n",
      "Cost: 0.45486795776552996\n",
      "Cost: 0.4548679577644414\n",
      "Cost: 0.454867957763803\n",
      "Cost: 0.4548679577630263\n",
      "Cost: 0.4548679577627469\n",
      "Cost: 0.45486795776158695\n",
      "Cost: 0.454867957762107\n",
      "Cost: 0.45486795776090266\n",
      "Cost: 0.45486795776180233\n",
      "Cost: 0.454867957762079\n",
      "Cost: 0.45486795775878147\n",
      "Cost: 0.4548679577620537\n",
      "Cost: 0.45486795775883926\n",
      "Cost: 0.4548679577623337\n",
      "Cost: 0.454867957763162\n",
      "Cost: 0.45486795776356814\n",
      "Cost: 0.45486795776349875\n",
      "Cost: 0.45486795775845246\n",
      "Cost: 0.4548679577642475\n",
      "Cost: 0.4548679577640179\n",
      "Cost: 0.45486795776273986\n",
      "Cost: 0.4548679577632058\n",
      "Cost: 0.454867957761583\n",
      "Cost: 0.45486795776014305\n",
      "Cost: 0.45486795776198236\n",
      "Cost: 0.4548679577579605\n",
      "Cost: 0.4548679577607494\n",
      "Cost: 0.45486795775982447\n",
      "Cost: 0.4548679577610648\n",
      "Cost: 0.45486795776106975\n",
      "Cost: 0.45486795776034267\n",
      "Cost: 0.4548679577616386\n",
      "Cost: 0.454867957757825\n",
      "Cost: 0.4548679577630357\n",
      "Cost: 0.4548679577580653\n",
      "Cost: 0.454867957762233\n",
      "Cost: 0.4548679577572067\n",
      "Cost: 0.45486795776264827\n",
      "Cost: 0.45486795776281125\n",
      "Cost: 0.45486795776230465\n",
      "Cost: 0.4548679577604342\n",
      "Cost: 0.4548679577628813\n",
      "Cost: 0.4548679577625112\n",
      "Cost: 0.45486795775795624\n",
      "Cost: 0.4548679577617303\n",
      "Cost: 0.45486795775982336\n",
      "Cost: 0.4547713590472258\n",
      "Cost: 0.4547957095487287\n",
      "Cost: 0.4547744385896385\n",
      "Cost: 0.4547701227493214\n",
      "Cost: 0.45477165356363103\n",
      "Cost: 0.45477072324984713\n",
      "Cost: 0.4547703807353065\n",
      "Cost: 0.45477024104525576\n",
      "Cost: 0.4547701792065698\n",
      "Cost: 0.45477015030320334\n",
      "Cost: 0.45477013635731933\n",
      "Cost: 0.45477012951105233\n",
      "Cost: 0.4547701261196161\n",
      "Cost: 0.45477012443182674\n",
      "Cost: 0.454770123589912\n",
      "Cost: 0.45477012316945037\n",
      "Cost: 0.4547701229593438\n",
      "Cost: 0.45477012285432206\n",
      "Cost: 0.45477012280181844\n",
      "Cost: 0.4547701227755697\n",
      "Cost: 0.4547701227624461\n",
      "Cost: 0.4547701227558821\n",
      "Cost: 0.45477012275260265\n",
      "Cost: 0.4547701227509612\n",
      "Cost: 0.45477012272826456\n",
      "Cost: 0.45477012274314815\n",
      "Cost: 0.45477012274984907\n",
      "Cost: 0.4547706840087702\n",
      "Cost: 0.45476716864078814\n",
      "Cost: 0.4547681325565889\n",
      "Cost: 0.4547674502817288\n",
      "Cost: 0.45476725914853366\n",
      "Cost: 0.4547672012873468\n",
      "Cost: 0.454767181808601\n",
      "Cost: 0.45476717443537357\n",
      "Cost: 0.45476717134069544\n",
      "Cost: 0.45476716994138733\n",
      "Cost: 0.45476716927874733\n",
      "Cost: 0.4547671689566828\n",
      "Cost: 0.4547671687979633\n",
      "Cost: 0.4547671687191822\n",
      "Cost: 0.45476716867993777\n",
      "Cost: 0.45476716866035044\n",
      "Cost: 0.45476716865056555\n",
      "Cost: 0.4547671686456769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.45476716864323263\n",
      "Cost: 0.4547671686420091\n",
      "Cost: 0.454767168641399\n",
      "Cost: 0.4547671686410921\n",
      "Cost: 0.4547671686378685\n",
      "Cost: 0.45476716863897504\n",
      "Cost: 0.4547671686396713\n",
      "Cost: 0.4547671686372192\n",
      "Cost: 0.4547671686383024\n",
      "Cost: 0.45476716864032785\n",
      "Cost: 0.45476755420195214\n",
      "Cost: 0.4547649015865062\n",
      "Cost: 0.45476561628775564\n",
      "Cost: 0.4547651056151443\n",
      "Cost: 0.4547649652178927\n",
      "Cost: 0.4547649237999209\n",
      "Cost: 0.4547649102918238\n",
      "Cost: 0.45476490533871555\n",
      "Cost: 0.4547649033124861\n",
      "Cost: 0.4547649024119623\n",
      "Cost: 0.45476490198985015\n",
      "Cost: 0.454764901785832\n",
      "Cost: 0.45476490168558226\n",
      "Cost: 0.45476490163589767\n",
      "Cost: 0.45476490161116545\n",
      "Cost: 0.45476490159882543\n",
      "Cost: 0.45476490159266525\n",
      "Cost: 0.45476490158958494\n",
      "Cost: 0.4547649015880446\n",
      "Cost: 0.4547649015872749\n",
      "Cost: 0.45476490158689065\n",
      "Cost: 0.4547649015866987\n",
      "Cost: 0.45476490158539684\n",
      "Cost: 0.45476490158566096\n",
      "Cost: 0.45476490158661964\n",
      "Cost: 0.45476490158674854\n",
      "Cost: 0.4547649015858756\n",
      "Cost: 0.454764901585061\n",
      "Cost: 0.45476490158557725\n",
      "Cost: 0.4547682475064768\n",
      "Cost: 0.45476502037866484\n",
      "Cost: 0.45476456939099025\n",
      "Cost: 0.45476469736728176\n",
      "Cost: 0.4547646089521421\n",
      "Cost: 0.45476458305887246\n",
      "Cost: 0.45476457469601483\n",
      "Cost: 0.4547645716611811\n",
      "Cost: 0.4547645704304949\n",
      "Cost: 0.45476456988684205\n",
      "Cost: 0.45476456963294176\n",
      "Cost: 0.45476456951047173\n",
      "Cost: 0.45476456945035737\n",
      "Cost: 0.45476456942058163\n",
      "Cost: 0.45476456940576204\n",
      "Cost: 0.4547645693983706\n",
      "Cost: 0.45476456939467935\n",
      "Cost: 0.4547645693928345\n",
      "Cost: 0.454764569391913\n",
      "Cost: 0.4547645693914521\n",
      "Cost: 0.4547645693912198\n",
      "Cost: 0.4547645693911065\n",
      "Cost: 0.4547645693910484\n",
      "Cost: 0.45476456939200455\n",
      "Cost: 0.4547645693914343\n",
      "Cost: 0.45476456939076293\n",
      "Cost: 0.45476456939110355\n",
      "Cost: 0.4547645693910796\n",
      "Cost: 0.4547645693915056\n",
      "Cost: 0.45476456939115695\n",
      "Cost: 0.45477232036216675\n",
      "Cost: 0.4547667978392752\n",
      "Cost: 0.45476526791746974\n",
      "Cost: 0.45476481423002807\n",
      "Cost: 0.45476466564154805\n",
      "Cost: 0.45476461096613935\n",
      "Cost: 0.4547645885400453\n",
      "Cost: 0.45476457855576446\n",
      "Cost: 0.4547645738709243\n",
      "Cost: 0.454764571605342\n",
      "Cost: 0.4547645704917609\n",
      "Cost: 0.454764569939775\n",
      "Cost: 0.45476456966498235\n",
      "Cost: 0.4547645695278859\n",
      "Cost: 0.4547645694594126\n",
      "Cost: 0.45476456942519616\n",
      "Cost: 0.4547645694080914\n",
      "Cost: 0.45476456939954113\n",
      "Cost: 0.4547645693952645\n",
      "Cost: 0.45476456939312837\n",
      "Cost: 0.4547645693920589\n",
      "Cost: 0.45476456939152454\n",
      "Cost: 0.454764569391259\n",
      "Cost: 0.45476456939112414\n",
      "Cost: 0.45476456939105825\n",
      "Cost: 0.4547645693910234\n",
      "Cost: 0.45476456939100623\n",
      "Cost: 0.4547645693909975\n",
      "Cost: 0.45476456939099497\n",
      "Cost: 0.45476456939099186\n",
      "Cost: 0.4547645693909919\n",
      "Cost: 0.45476456939099075\n",
      "Cost: 0.45476456939098964\n",
      "Cost: 0.4547645693909892\n",
      "Cost: 0.4547645693909901\n",
      "Cost: 0.45476456939099025\n",
      "Cost: 0.4547645693909903\n",
      "Cost: 0.4547645693909892\n",
      "Training Result: 0.4547645693909892\n",
      "Theta1 new: [[ 1.89442328e+00 -4.47283334e-13 -6.51727578e-11 ...  5.09326775e-05\n",
      "  -6.50843359e-11  3.08228162e-11]\n",
      " [ 5.04878625e-01 -3.05099844e-11 -5.68655500e-11 ...  7.63379165e-05\n",
      "   3.76613275e-11 -5.32544778e-11]\n",
      " [ 8.46339788e-01  3.38788964e-11  2.27246929e-11 ...  2.75344153e-05\n",
      "   8.39673356e-12  6.84952848e-13]\n",
      " ...\n",
      " [-1.87904139e+00  3.73203122e-11  5.59427458e-11 ... -9.00217719e-07\n",
      "  -5.67036319e-11  2.69572576e-12]\n",
      " [-1.65305914e+00 -6.44396695e-11 -3.58685585e-11 ... -3.51095131e-05\n",
      "   3.12616786e-11  1.18690078e-11]\n",
      " [-4.91371520e-01 -3.73593806e-11  2.85303205e-11 ...  1.81638719e-05\n",
      "   5.15623718e-11 -5.10737847e-11]]\n",
      "Theta2 new: [[-1.73823422  0.52523597  0.71790114 ... -1.13080366 -0.26856621\n",
      "   0.14475985]\n",
      " [-6.6663824  -1.46102863  0.70179282 ...  0.09601089 -0.07217847\n",
      "  -0.17571253]\n",
      " [-1.89619116 -0.43318412 -1.20452416 ... -0.90902892 -0.07332183\n",
      "  -0.06629211]\n",
      " ...\n",
      " [-3.86671895  1.43487178  1.1428555  ...  0.94563843 -0.07081071\n",
      "   0.00729768]\n",
      " [-1.14896332 -0.01903258  0.41223283 ...  1.20358619 -0.09085718\n",
      "  -0.03076459]\n",
      " [-4.12516308 -0.65573125 -0.74704281 ...  0.31009694  0.07354455\n",
      "  -0.60384747]]\n",
      "Test Cost: 0.7118759564056001\n",
      "[1.10929571e-05 9.97732217e-01 1.84838247e-02 4.83987323e-06\n",
      " 8.89536080e-05 6.21471866e-05 4.29442449e-04 1.19148975e-03\n",
      " 3.33698861e-05 1.08568774e-02]\n",
      "Predicted Value is: 2\n"
     ]
    }
   ],
   "source": [
    "flattened_params = np.hstack((theta1.flatten(),theta2.flatten()))\n",
    "lambda_val = 2\n",
    "#calculate the gradients and cost\n",
    "final_res = n.nnCostFunction(flattened_params, input_layer_size, hidden_layer_size, num_labels, x, y, lambda_val)\n",
    "#Minimize:\n",
    "#flatten and merge theta1 and theta2 values into a single vector \n",
    "nn_params = flattened_params\n",
    "func_args = (input_layer_size, hidden_layer_size, num_labels, x, y, lambda_val)\n",
    "#minimize using the conjugate-gradient (cg) algorithm \n",
    "result = sp.minimize(n.nnCostFunction, x0 = nn_params, args = func_args, method = 'tnc', jac = True, options = {'disp': True, 'maxiter': 1000})\n",
    "print(\"Training Result: \" + str(result.fun))\n",
    "adjusted_weights = result.x\n",
    "theta1 = np.reshape(adjusted_weights[:hidden_layer_size*(input_layer_size+1)],(hidden_layer_size, input_layer_size+1))\n",
    "theta2 = np.reshape(adjusted_weights[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "print (\"Theta1 new: \" + str(theta1))\n",
    "print(\"Theta2 new: \" + str(theta2))\n",
    "#Prediction: Training Data\n",
    "J = 0\n",
    "cost_temp = 0\n",
    "#Cost: Test Data\n",
    "for samp in range(len(x_test)):\n",
    "    x_curr = x_test[samp]\n",
    "    z2 = np.matmul(x_curr,theta1.T)\n",
    "    a2 = n.sigmoid(z2)\n",
    "    a2 = np.hstack(([1], a2))\n",
    "    z3 = np.matmul(a2,theta2.T)\n",
    "    a3 = n.sigmoid(z3)\n",
    "\n",
    "    cost_temp += n.calc_cost(y_test[samp], a3, lambda_val, len(x_test))\n",
    "\n",
    "#Accumulate cost values and regularize to get Cost(J) \n",
    "term1 = (1/len(x_test))*cost_temp\n",
    "term2 = n.regularization(1, len(x_test))\n",
    "J = term1 + term2\n",
    "print(\"Test Cost: \" + str(J))\n",
    "\n",
    "print(\"Predicted Value is: \" + str(n.predict([theta1,theta2],x[2235])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ec935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32586142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
