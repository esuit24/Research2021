{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34fb3a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1500, 10)\n",
      "Lambda Val: 0\n",
      "Training Cost: 0.2923452955047028\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.230121\n",
      "         Iterations: 5\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "[3.38994641e-04 1.30548366e-03 1.42211564e-03 1.61309483e-04\n",
      " 1.43774962e-03 1.35098549e-04 1.03473203e-05 9.98004288e-01\n",
      " 3.88425071e-03 1.08913801e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.7168407875883087\n",
      "Lambda Val: 0.1\n",
      "Training Cost: 0.3053106185454638\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.250433\n",
      "         Iterations: 5\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "[3.57573589e-04 1.92047716e-03 1.23898799e-03 1.72440598e-04\n",
      " 1.72313800e-03 1.70712561e-04 1.21203205e-05 9.97646058e-01\n",
      " 3.72409638e-03 1.24672777e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6952333708954693\n",
      "Lambda Val: 0.2\n",
      "Training Cost: 0.3182759415862249\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.269472\n",
      "         Iterations: 5\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "[3.86565907e-04 2.25480699e-03 1.35637190e-03 1.78532564e-04\n",
      " 1.79538901e-03 1.92496022e-04 1.35637371e-05 9.97279389e-01\n",
      " 3.69772878e-03 1.32889665e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6887157590001884\n",
      "Lambda Val: 0.30000000000000004\n",
      "Training Cost: 0.33124126462698594\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.284661\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "[3.83450656e-04 2.64894226e-03 1.00948731e-03 1.63826670e-04\n",
      " 1.83415946e-03 1.76401778e-04 1.33609758e-05 9.96479361e-01\n",
      " 2.84525905e-03 1.20658596e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6899603254266421\n",
      "Lambda Val: 0.4\n",
      "Training Cost: 0.344206587667747\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.301623\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "[4.24236014e-04 2.52566896e-03 1.28343154e-03 1.71849227e-04\n",
      " 1.78376896e-03 1.81440282e-04 1.39371974e-05 9.96005596e-01\n",
      " 2.88521035e-03 1.20468840e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6921961530057008\n",
      "Lambda Val: 0.5\n",
      "Training Cost: 0.35717191070850807\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.333965\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[5.37544910e-04 3.40789871e-03 2.42814275e-03 3.81429822e-04\n",
      " 2.64101304e-03 2.76043965e-04 3.45582427e-05 9.94812956e-01\n",
      " 5.35143250e-03 1.62323775e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6144201145537127\n",
      "Lambda Val: 0.6\n",
      "Training Cost: 0.3701372337492691\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.348775\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[5.49041894e-04 3.43129086e-03 2.49217479e-03 3.89051486e-04\n",
      " 2.66518010e-03 2.80623715e-04 3.57389052e-05 9.94738719e-01\n",
      " 5.44316726e-03 1.64663776e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6144479273650275\n",
      "Lambda Val: 0.7\n",
      "Training Cost: 0.38310255679003014\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.363742\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[5.63176157e-04 3.42314175e-03 2.63280219e-03 4.03415953e-04\n",
      " 2.69311179e-03 2.84819946e-04 3.75242799e-05 9.94651949e-01\n",
      " 5.59234909e-03 1.67399690e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6140110362023558\n",
      "Lambda Val: 0.7999999999999999\n",
      "Training Cost: 0.39606787983079117\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.375971\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[5.83036572e-04 1.98081628e-03 3.19907725e-03 4.31562387e-04\n",
      " 2.50716445e-03 2.25349233e-04 3.27590957e-05 9.94884551e-01\n",
      " 5.60467616e-03 1.41540357e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6372201446331276\n",
      "Lambda Val: 0.8999999999999999\n",
      "Training Cost: 0.4090332028715522\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.391174\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[6.06057761e-04 2.10337731e-03 3.31112313e-03 4.44873657e-04\n",
      " 2.59469745e-03 2.39318615e-04 3.45438908e-05 9.94716575e-01\n",
      " 5.74307135e-03 1.46996713e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6341695142266934\n",
      "Lambda Val: 0.9999999999999999\n",
      "Training Cost: 0.42199852591231324\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.406124\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[6.28029323e-04 2.23205660e-03 3.41597806e-03 4.57978153e-04\n",
      " 2.67962477e-03 2.53244982e-04 3.63874641e-05 9.94541873e-01\n",
      " 5.87485051e-03 1.52293265e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6311381555533562\n",
      "Lambda Val: 1.0999999999999999\n",
      "Training Cost: 0.43496384895307433\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.420834\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[6.48768642e-04 2.36557744e-03 3.51303990e-03 4.70700467e-04\n",
      " 2.76102436e-03 2.66961077e-04 3.82675288e-05 9.94361727e-01\n",
      " 5.99835019e-03 1.57371152e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6281726176398332\n",
      "Lambda Val: 1.2\n",
      "Training Cost: 0.4479291719938354\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.435317\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[6.68164845e-04 2.50213161e-03 3.60213839e-03 4.82890705e-04\n",
      " 2.83816249e-03 2.80306874e-04 4.01584165e-05 9.94177978e-01\n",
      " 6.11244629e-03 1.62178952e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6253223129124346\n",
      "Lambda Val: 1.3\n",
      "Training Cost: 0.46089449503459645\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.449589\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[6.86189236e-04 2.63941069e-03 3.68348463e-03 4.94439400e-04\n",
      " 2.91055546e-03 2.93139213e-04 4.20331247e-05 9.93992986e-01\n",
      " 6.21666526e-03 1.66676724e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6226365399377533\n",
      "Lambda Val: 1.4000000000000001\n",
      "Training Cost: 0.4738598180753575\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.463668\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[7.02899852e-04 2.77469967e-03 3.75761382e-03 5.05290913e-04\n",
      " 2.97800264e-03 3.05339479e-04 4.38661471e-05 9.93809465e-01\n",
      " 6.31122788e-03 1.70839080e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6201615925331959\n",
      "Lambda Val: 1.5000000000000002\n",
      "Training Cost: 0.4868251411161186\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.477571\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[7.18440226e-04 2.90503203e-03 3.82533720e-03 5.15453527e-04\n",
      " 3.04058935e-03 3.16818693e-04 4.56367621e-05 9.93630215e-01\n",
      " 6.39702160e-03 1.74656926e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6179382846424539\n",
      "Lambda Val: 1.6000000000000003\n",
      "Training Cost: 0.49979046415687967\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.491312\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[7.33033222e-04 3.02739479e-03 3.88770778e-03 5.25004434e-04\n",
      " 3.09866470e-03 3.27520235e-04 4.73322300e-05 9.93457777e-01\n",
      " 6.47550928e-03 1.78137740e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6160001607416377\n",
      "Lambda Val: 1.7000000000000004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost: 0.5127557871976407\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.504906\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[7.46971404e-04 3.13895959e-03 3.94599410e-03 5.34088385e-04\n",
      " 3.15280500e-03 3.37420998e-04 4.89502451e-05 9.93294082e-01\n",
      " 6.54859402e-03 1.81304506e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6143725826188722\n",
      "Lambda Val: 1.8000000000000005\n",
      "Training Cost: 0.5257211102384017\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.518365\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[7.60605862e-04 3.23730522e-03 4.00165251e-03 5.42909982e-04\n",
      " 3.20377467e-03 3.46531954e-04 5.05000995e-05 9.93140175e-01\n",
      " 6.61846729e-03 1.84193647e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6130727733278951\n",
      "Lambda Val: 1.9000000000000006\n",
      "Training Cost: 0.5386864332791628\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.531700\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[7.74335480e-04 3.32059521e-03 4.05629150e-03 5.51721147e-04\n",
      " 3.25249374e-03 3.54898686e-04 5.20023607e-05 9.92996063e-01\n",
      " 6.68746844e-03 1.86852366e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6121107424197687\n",
      "Lambda Val: 2.0000000000000004\n",
      "Training Cost: 0.5516517563199239\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.544918\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[7.88598072e-04 3.38768549e-03 4.11162772e-03 5.60806453e-04\n",
      " 3.30001522e-03 3.62601787e-04 5.34873141e-05 9.92860712e-01\n",
      " 6.75797431e-03 1.89335816e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6114908272443896\n",
      "Lambda Val: 2.1000000000000005\n",
      "Training Cost: 0.564617079360685\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.558606\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[7.82282470e-04 4.38922351e-03 3.26371926e-03 5.00176435e-04\n",
      " 3.59972967e-03 4.23401984e-04 4.75570367e-05 9.92760392e-01\n",
      " 6.30782223e-03 1.95233761e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6042419124051034\n",
      "Lambda Val: 2.2000000000000006\n",
      "Training Cost: 0.577582402401446\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.573157\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "[7.67221084e-04 4.28603120e-03 4.64512896e-03 5.37734767e-04\n",
      " 3.26867312e-03 4.09721183e-04 5.30521954e-05 9.92171680e-01\n",
      " 6.72611246e-03 1.95310148e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.5958357437644963\n",
      "Lambda Val: 2.3000000000000007\n",
      "Training Cost: 0.590547725442207\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.585680\n",
      "         Iterations: 5\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 16\n",
      "[8.00488000e-04 3.77189699e-03 4.93035042e-03 5.77787398e-04\n",
      " 3.35305503e-03 3.99394392e-04 5.41063737e-05 9.92298628e-01\n",
      " 7.05247822e-03 1.93695473e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.5992620058305703\n",
      "Lambda Val: 2.400000000000001\n",
      "Training Cost: 0.603513048482968\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.598499\n",
      "         Iterations: 5\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 16\n",
      "[8.11377760e-04 3.78122927e-03 4.96873023e-03 5.84078681e-04\n",
      " 3.38698431e-03 4.03906074e-04 5.48650785e-05 9.92234580e-01\n",
      " 7.10866243e-03 1.95045575e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.5994747538536521\n",
      "Lambda Val: 2.500000000000001\n",
      "Training Cost: 0.6164783715237292\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.611223\n",
      "         Iterations: 5\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 16\n",
      "[8.23933536e-04 3.78289860e-03 5.00984621e-03 5.90914416e-04\n",
      " 3.42422449e-03 4.08685815e-04 5.56597984e-05 9.92172231e-01\n",
      " 7.17159865e-03 1.96513678e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.5998929359391021\n",
      "Lambda Val: 2.600000000000001\n",
      "Training Cost: 0.6294436945644902\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.623834\n",
      "         Iterations: 5\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 16\n",
      "[8.38847966e-04 3.77654184e-03 5.05315605e-03 5.98525697e-04\n",
      " 3.46672700e-03 4.13980488e-04 5.65218311e-05 9.92112251e-01\n",
      " 7.24455748e-03 1.98194724e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6005768591243167\n",
      "Lambda Val: 2.700000000000001\n",
      "Training Cost: 0.6424090176052513\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.635271\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[9.07230660e-04 3.82075518e-03 5.07698883e-03 5.80313141e-04\n",
      " 3.59924033e-03 4.52743437e-04 5.53853550e-05 9.92291082e-01\n",
      " 7.40964446e-03 2.09402230e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6094666214299655\n",
      "Lambda Val: 2.800000000000001\n",
      "Training Cost: 0.6553743406460124\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.647675\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[9.15289561e-04 3.86409314e-03 5.03575312e-03 5.92331585e-04\n",
      " 3.64299511e-03 4.55437638e-04 5.72918315e-05 9.92203333e-01\n",
      " 7.48376449e-03 2.11172827e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6089030377755238\n",
      "Lambda Val: 2.9000000000000012\n",
      "Training Cost: 0.6683396636867734\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.659970\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[9.25462170e-04 3.89380717e-03 5.00328891e-03 6.06736481e-04\n",
      " 3.68718469e-03 4.57194772e-04 5.96020506e-05 9.92100361e-01\n",
      " 7.56674271e-03 2.12983300e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6085956965982813\n",
      "Lambda Val: 3.0000000000000013\n",
      "Training Cost: 0.6813049867275345\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.673223\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n",
      "[9.51987569e-04 3.84436021e-03 5.73380513e-03 6.78306083e-04\n",
      " 3.31640768e-03 4.01081581e-04 8.49205112e-05 9.90672331e-01\n",
      " 7.60543589e-03 2.16070956e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.615275527062648\n",
      "Lambda Val: 3.1000000000000014\n",
      "Training Cost: 0.6942703097682955\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.684228\n",
      "         Iterations: 5\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "[1.03109536e-03 3.48900273e-03 5.73161561e-03 7.24201619e-04\n",
      " 3.67666533e-03 4.27532457e-04 8.09066411e-05 9.91023694e-01\n",
      " 8.08604601e-03 2.17571350e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6178652588652598\n",
      "Lambda Val: 3.2000000000000015\n",
      "Training Cost: 0.7072356328090565\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.695879\n",
      "         Iterations: 5\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "[1.00378333e-03 3.78043610e-03 5.08545109e-03 6.84808445e-04\n",
      " 3.89683590e-03 4.61908018e-04 7.10496296e-05 9.91597536e-01\n",
      " 7.99971722e-03 2.20049565e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6118076887041962\n",
      "Lambda Val: 3.3000000000000016\n",
      "Training Cost: 0.7202009558498177\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.708468\n",
      "         Iterations: 5\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "[9.73945037e-04 4.15839858e-03 4.57734803e-03 6.44377180e-04\n",
      " 4.01572893e-03 4.89425924e-04 6.46540476e-05 9.91892904e-01\n",
      " 7.80427292e-03 2.21461364e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6071619551331562\n",
      "Lambda Val: 3.4000000000000017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Cost: 0.7331662788905787\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.718591\n",
      "         Iterations: 5\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "[1.14429904e-03 3.61183357e-03 5.16021850e-03 7.81118498e-04\n",
      " 4.26281815e-03 4.83571203e-04 8.52166881e-05 9.90970584e-01\n",
      " 8.51914732e-03 2.30613188e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6204136369176005\n",
      "Lambda Val: 3.5000000000000018\n",
      "Training Cost: 0.7461316019313398\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.729265\n",
      "         Iterations: 5\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "[1.20853596e-03 3.77681246e-03 5.05390930e-03 8.01564216e-04\n",
      " 4.42168443e-03 5.04984833e-04 9.14030516e-05 9.90445225e-01\n",
      " 8.50552328e-03 2.37505026e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6233195768102915\n",
      "Lambda Val: 3.600000000000002\n",
      "Training Cost: 0.7590969249721008\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.739902\n",
      "         Iterations: 5\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "[1.29862893e-03 3.84638285e-03 5.35060409e-03 8.42922535e-04\n",
      " 4.51826154e-03 5.19511172e-04 1.01582091e-04 9.89500903e-01\n",
      " 8.51305932e-03 2.43434363e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6277198652096327\n",
      "Lambda Val: 3.700000000000002\n",
      "Training Cost: 0.7720622480128618\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.753225\n",
      "         Iterations: 5\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "[1.06811849e-03 5.06382149e-03 4.24050620e-03 6.88581545e-04\n",
      " 4.20557959e-03 5.19633011e-04 8.54947383e-05 9.90297148e-01\n",
      " 7.56255080e-03 2.39548633e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6111150741406761\n",
      "Lambda Val: 3.800000000000002\n",
      "Training Cost: 0.785027571053623\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.764252\n",
      "         Iterations: 5\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "[1.10283221e-03 5.06650699e-03 4.19514848e-03 7.25874120e-04\n",
      " 4.36470661e-03 5.27020710e-04 9.18514841e-05 9.90207088e-01\n",
      " 7.79191858e-03 2.44671698e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6124296214336702\n",
      "Lambda Val: 3.900000000000002\n",
      "Training Cost: 0.797992894094384\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.775432\n",
      "         Iterations: 5\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "[1.14300303e-03 4.87798233e-03 4.26016503e-03 7.94291839e-04\n",
      " 4.53893456e-03 5.21912026e-04 1.01946827e-04 9.90106062e-01\n",
      " 8.20003633e-03 2.47935183e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6137625608319492\n",
      "Lambda Val: 4.000000000000002\n",
      "Training Cost: 0.810958217135145\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.787444\n",
      "         Iterations: 5\n",
      "         Function evaluations: 13\n",
      "         Gradient evaluations: 13\n",
      "[1.20200047e-03 4.37961119e-03 4.74338349e-03 9.24209074e-04\n",
      " 4.66369262e-03 4.93793883e-04 1.20035899e-04 9.89635796e-01\n",
      " 8.83643424e-03 2.45677247e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6161458543633733\n",
      "Lambda Val: 4.100000000000001\n",
      "Training Cost: 0.823923540175906\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.799839\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "[1.27621088e-03 3.91484209e-03 7.02456722e-03 9.18837947e-04\n",
      " 4.26534372e-03 4.96541786e-04 1.16268493e-04 9.89141624e-01\n",
      " 9.29333705e-03 2.47556772e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6199767210906644\n",
      "Lambda Val: 4.200000000000001\n",
      "Training Cost: 0.8368888632166671\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.809102\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "[1.23100594e-03 4.61341584e-03 5.08342596e-03 8.59682237e-04\n",
      " 4.76811092e-03 5.52363398e-04 1.05867486e-04 9.90126297e-01\n",
      " 9.06010023e-03 2.57755973e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6156478596211978\n",
      "Lambda Val: 4.300000000000001\n",
      "Training Cost: 0.849854186257428\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.819914\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "[1.27346016e-03 4.57913294e-03 5.47228826e-03 8.69936183e-04\n",
      " 4.81934084e-03 5.69656136e-04 1.06171694e-04 9.89969721e-01\n",
      " 9.25988461e-03 2.61689255e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6169364050292538\n",
      "Lambda Val: 4.4\n",
      "Training Cost: 0.8628195092981891\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.830580\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "[1.32253543e-03 4.55689902e-03 5.86848469e-03 8.82017679e-04\n",
      " 4.88263233e-03 5.88865650e-04 1.07066810e-04 9.89763063e-01\n",
      " 9.45357503e-03 2.65936938e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6185410193638187\n",
      "Lambda Val: 4.5\n",
      "Training Cost: 0.87578483233895\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.841119\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "[1.37646662e-03 4.54251946e-03 6.25831607e-03 8.96284800e-04\n",
      " 4.96070749e-03 6.09655559e-04 1.08417804e-04 9.89533310e-01\n",
      " 9.65077961e-03 2.70424514e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6203314383790093\n",
      "Lambda Val: 4.6\n",
      "Training Cost: 0.8887501553797111\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.851546\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "[1.43390423e-03 4.53244247e-03 6.63731593e-03 9.12679702e-04\n",
      " 5.05286098e-03 6.31679895e-04 1.10086867e-04 9.89295414e-01\n",
      " 9.85549800e-03 2.75057016e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6222088484733281\n",
      "Lambda Val: 4.699999999999999\n",
      "Training Cost: 0.9017154784204721\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.861867\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "[1.49489718e-03 4.51928672e-03 7.01941594e-03 9.31877325e-04\n",
      " 5.15646274e-03 6.54366236e-04 1.12073644e-04 9.89046374e-01\n",
      " 1.00711065e-02 2.79704594e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6241424467514242\n",
      "Lambda Val: 4.799999999999999\n",
      "Training Cost: 0.9146808014612331\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.872092\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n",
      "[1.55900428e-03 4.50276163e-03 7.40787678e-03 9.53393529e-04\n",
      " 5.26957609e-03 6.77667579e-04 1.14268993e-04 9.88789386e-01\n",
      " 1.02962080e-02 2.84330621e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6261039834340563\n",
      "Lambda Val: 4.899999999999999\n",
      "Training Cost: 0.927646124501994\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.874401\n",
      "         Iterations: 5\n",
      "         Function evaluations: 16\n",
      "         Gradient evaluations: 16\n",
      "[2.36769344e-03 4.75478061e-03 8.44766027e-03 1.22132309e-03\n",
      " 7.19184560e-03 9.58869922e-04 1.56663492e-04 9.87653621e-01\n",
      " 1.28057236e-02 3.58965929e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6616754863333523\n",
      "Lambda Val: 4.999999999999998\n",
      "Training Cost: 0.9406114475427552\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.877465\n",
      "         Iterations: 5\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 15\n",
      "[2.44914485e-03 6.68701542e-03 5.88776049e-03 1.31488109e-03\n",
      " 6.84646423e-03 9.55600614e-04 2.39698546e-04 9.82800580e-01\n",
      " 1.04890453e-02 3.73552491e-03]\n",
      "Predicted Value is: 8\n",
      "Actual Value is: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "Test Cost: 0.6591978616368365\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize as sp\n",
    "import pandas as pd\n",
    "import math\n",
    "from random import sample, uniform\n",
    "\n",
    "class Neural_Network():\n",
    "    #calculates sigmoid function, params: z (variable), returns: sigmoid calculation\n",
    "    def sigmoid(self,z):\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    #calculates the sigmoidGradient (derivative), params: z (variable), returns: sigmoid gradient calculation \n",
    "    def sigmoidGradient(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "    \n",
    "    def random_initialization(self, num_in, num_out):\n",
    "        epsilon = 0.12\n",
    "        all_weights = uniform(-epsilon, epsilon, weights.size)\n",
    "        weights = np.reshape(all_weights,(num_out,num_in+1))\n",
    "        return weights\n",
    "    \n",
    "    #calculates the regularization term for the neural network, params: lambda (regularization constant),\n",
    "    #m (number of training samples), returns regularization value\n",
    "    def regularization(self, lamda, m):\n",
    "        lamda_val = lamda/(2.0*m)\n",
    "        theta1_sum = 0 \n",
    "        theta2_sum = 0\n",
    "        for j in range(len(self.Theta1)-1):\n",
    "            for k in range(self.Theta1[0].size-1):\n",
    "                theta1_sum += self.Theta1[j+1][k+1]*self.Theta1[j+1][k+1]\n",
    "        for j in range(len(self.Theta2)-1):\n",
    "            for k in range(self.Theta2[0].size-1):\n",
    "                theta2_sum += self.Theta2[j+1][k+1]*self.Theta2[j+1][k+1]\n",
    "        return lamda_val*(theta1_sum+theta2_sum)\n",
    "    \n",
    "    #calculates the cost for the neural network, params: y_vals (expected output values), hyp (calculated output values),\n",
    "    #m (number of training samples), returns cost between given sample and expected value  \n",
    "    def calc_cost(self, y_vals, hyp, lamda, m): #hyp and y are both 10x1 vectors \n",
    "        cost = 0\n",
    "        for k in range(len(y_vals)):\n",
    "            cost += -y_vals[k] * math.log(hyp[k]) - (1-y_vals[k])*math.log(1-hyp[k])\n",
    "        return cost\n",
    "    \n",
    "    #predicts the number that correlates to the input data, params: weights(an array that consists of 2 weight matricies),\n",
    "    #x_vals (array that consists of input values), returns prediction number (0-9) \n",
    "    def predict(self, weights, x_vals):\n",
    "            #x_vals = np.hstack(([1],x_vals))\n",
    "            weights1 = weights[0]\n",
    "            weights2 = weights[1]\n",
    "            z2 = np.matmul(x_vals,weights1.T)\n",
    "            a2 = self.sigmoid(z2)\n",
    "            a2 = np.hstack(([1], a2))\n",
    "            z3 = np.matmul(a2,weights2.T)\n",
    "            a3 = self.sigmoid(z3)\n",
    "            max_val = a3[0]\n",
    "            max_index = 0\n",
    "            print(a3)\n",
    "            for i in range(len(a3)):\n",
    "                if (a3[i] > max_val):\n",
    "                    max_val = a3[i]\n",
    "                    max_index = i\n",
    "            prediction = max_index+1\n",
    "            if prediction == 10:\n",
    "                prediction = 0\n",
    "            return prediction\n",
    "        \n",
    "    #performs forward and backward prop to get a final cost value, J, and 2 gradient weight matricies\n",
    "    #params: nn_params(array that consists of 2 weight matricies for layer 1 and 2 respectively), input_layer_size (number of input units),\n",
    "    #hidden_layer_size (number of hidden units), num_labels (number of output units), x (training samples), y (expected output values), lambda_reg (regularization constant)\n",
    "    #returns cost and an array of weight gradient vectors \n",
    "    def nnCostFunction(self, nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_reg):\n",
    "        self.Theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)],(hidden_layer_size, input_layer_size+1))\n",
    "        self.Theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "        \n",
    "        J = 0;\n",
    "        Theta1_grad = np.zeros_like(self.Theta1)\n",
    "        Theta2_grad = np.zeros_like(self.Theta2)\n",
    "       \n",
    "        #Forward and Back prop: \n",
    "\n",
    "        bigDelta1 = 0\n",
    "        bigDelta2 = 0\n",
    "        cost_temp = 0\n",
    "\n",
    "        # for each training example\n",
    "        for t in range(m):\n",
    "\n",
    "            ## step 1: perform forward pass\n",
    "            x = X[t]\n",
    "\n",
    "            #calculate z2 (linear combination) and a2 (activation for layer 2)\n",
    "            z2 = np.matmul(x,self.Theta1.T)\n",
    "            a2 = self.sigmoid(z2)\n",
    "\n",
    "            # add column of ones as bias unit to the second layer\n",
    "            a2 = np.hstack(([1], a2))\n",
    "            # calculate z3 (linear combination) and a3 (activation for layer 3 aka final hypothesis)\n",
    "            z3 = np.matmul(a2,self.Theta2.T)\n",
    "            a3 = self.sigmoid(z3)\n",
    "            \n",
    "            #Backpropogation: \n",
    "\n",
    "            #step 2: set delta 3\n",
    "            delta3 = np.zeros((num_labels))\n",
    "\n",
    "            #Get Error: subtract actual val in y from each hypothesized val in a3  \n",
    "            y_vals = np.zeros((num_labels))\n",
    "            for k in range(num_labels): #for each of the 10 labels subtract\n",
    "                y_k = y[t][k]\n",
    "                y_vals[k] = y_k\n",
    "                delta3[k] = a3[k] - y_k\n",
    "\n",
    "            #step 3: for layer 2 set delta2 = Theta2 Transpose * delta3 .* sigmoidGradient(z2) (= Chain Rule)\n",
    "            #Skip over the bias unit in layer 2: no gradient calculated for this value \n",
    "            delta2 = np.matmul(self.Theta2[:,1:].T, delta3) * self.sigmoidGradient(z2)\n",
    "\n",
    "            #step 4: accumulate gradient from this sample\n",
    "            bigDelta1 += np.outer(delta2, x)\n",
    "            bigDelta2 += np.outer(delta3, a2)\n",
    "            #Update the total cost given the cost from this sample\n",
    "            cost_temp += self.calc_cost(y_vals, a3, lambda_reg, m)\n",
    "            \n",
    "        #Accumulate cost values and regularize to get Cost(J) \n",
    "        term1 = (1/m)*cost_temp\n",
    "        term2 = self.regularization(lambda_reg, m)\n",
    "        J = term1 + term2\n",
    "        #print(\"Cost: \" + str(J)) \n",
    "        \n",
    "        # step 5: obtain gradient for neural net cost function by dividing the accumulated gradients by m\n",
    "        Theta1_grad = bigDelta1 / m\n",
    "        Theta2_grad = bigDelta2 / m\n",
    "        \n",
    "\n",
    "        #Regularization\n",
    "        #only regularize for j >= 1, so skip the first column\n",
    "        Theta1_grad_unregularized = np.copy(Theta1_grad)\n",
    "        Theta2_grad_unregularized = np.copy(Theta2_grad)\n",
    "        Theta1_grad += (float(lambda_reg)/m)*self.Theta1\n",
    "        Theta2_grad += (float(lambda_reg)/m)*self.Theta2\n",
    "        Theta1_grad[:,0] = Theta1_grad_unregularized[:,0]\n",
    "        Theta2_grad[:,0] = Theta2_grad_unregularized[:,0]\n",
    "        flattened_grads = np.hstack((Theta1_grad.flatten(),Theta2_grad.flatten()))\n",
    "        \n",
    "        return J, flattened_grads\n",
    "        \n",
    "        \n",
    "#Read in data files\n",
    "df = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Theta1.csv', header = None)\n",
    "df2 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Theta2.csv', header = None)\n",
    "df3 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/X.csv', header = None)\n",
    "df4 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Y.csv', header = None)\n",
    "#Initialize layer sizes\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "#Set sizes for weight and data matricies\n",
    "theta1 = np.zeros([hidden_layer_size,input_layer_size+1])\n",
    "theta2 = np.zeros([num_labels,hidden_layer_size+1])\n",
    "x = np.zeros((len(df3),input_layer_size))\n",
    "x_sample = np.zeros((int(len(df3)*(0.7)), input_layer_size)) #take only 70% for training to leave 30% for testing\n",
    "y_vec = np.zeros((len(df4),))\n",
    "y_sample = np.zeros((int(len(df4)*0.7)),)\n",
    "random_indicies = sample(range(0,int(len(df3))),int(len(df3)*0.7)) \n",
    "\n",
    "#create data and weight arrays\n",
    "index = 0 \n",
    "while (index < hidden_layer_size):\n",
    "    theta1[index] = df.iloc[index]\n",
    "    index+=1\n",
    "\n",
    "index = 0\n",
    "while(index<num_labels):\n",
    "    theta2[index] = df2.iloc[index]\n",
    "    index+=1\n",
    "\n",
    "index = 0\n",
    "while(index<len(x)):\n",
    "    x[index] = df3.iloc[index]\n",
    "    index+=1\n",
    "\n",
    "index = 0\n",
    "while (index<len(y_vec)):\n",
    "    y_vec[index] = df4.iloc[index]\n",
    "    index+=1\n",
    "    \n",
    "for index in range(len(random_indicies)):\n",
    "    sample_index = random_indicies[index] \n",
    "    x_sample[index] = x[sample_index] \n",
    "    y_sample[index] = y_vec[sample_index]\n",
    "x_test = np.zeros((int(len(df3)*0.3),input_layer_size))\n",
    "y_test = np.zeros((int(len(df4)*0.3),))\n",
    "#set test data\n",
    "test_indicies = np.zeros((int(len(df3)*0.3),))\n",
    "count = 0\n",
    "for ind in range(len(df3)):\n",
    "    if ind not in random_indicies:\n",
    "        test_indicies[count] = ind\n",
    "        count+=1\n",
    "for ii in range(len(test_indicies)):\n",
    "    test_index = int(test_indicies[ii])\n",
    "    x_test[ii] = x[test_index]\n",
    "    y_test[ii] = y_vec[test_index]\n",
    "    \n",
    "x = x_sample\n",
    "ones = np.ones((len(x_sample),1))\n",
    "test_ones = np.ones((len(x_test),1))\n",
    "x = np.hstack((ones, x)) \n",
    "x_test = np.hstack((test_ones,x_test))\n",
    "y_vec = y_sample\n",
    "    \n",
    "m = len(x)\n",
    "# set y to be a 2-D matrix with each column being a different sample and each row corresponding to a value 0-9\n",
    "y = np.zeros((m,num_labels))\n",
    "y_test_matrix = np.zeros((len(y_test),num_labels))\n",
    "# for every label, convert it into vector of 0s and a 1 in the appropriate position\n",
    "for i in range(m): #each row is new training sample\n",
    "    index = int(y_vec[i]-1)\n",
    "    y[i][index] = 1\n",
    "y_temp = y_test\n",
    "for j in range(int(len(y_test))):\n",
    "    index2 = int(y_temp[j]-1)\n",
    "    y_test_matrix[j][index2] = 1\n",
    "y_test = y_test_matrix\n",
    "nn_params = [theta1, theta2] \n",
    "flattened_params = np.hstack((theta1.flatten(),theta2.flatten()))\n",
    "lambda_val = 2.3\n",
    "n = Neural_Network()\n",
    "#calculate the gradients and cost\n",
    "final_res = n.nnCostFunction(flattened_params, input_layer_size, hidden_layer_size, num_labels, x, y, lambda_val)\n",
    "#Minimize:\n",
    "#flatten and merge theta1 and theta2 values into a single vector \n",
    "nn_params = flattened_params\n",
    "func_args = (input_layer_size, hidden_layer_size, num_labels, x, y, lambda_val)\n",
    "#minimize using the conjugate-gradient (cg) algorithm \n",
    "result = sp.minimize(n.nnCostFunction, x0 = nn_params, args = func_args, method = 'cg', jac = True, options = {'disp': True, 'maxiter': 5})\n",
    "adjusted_weights = result.x\n",
    "theta1 = np.reshape(adjusted_weights[:hidden_layer_size*(input_layer_size+1)],(hidden_layer_size, input_layer_size+1))\n",
    "theta2 = np.reshape(adjusted_weights[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "\n",
    "#Prediction: Training Data\n",
    "J = 0\n",
    "cost_temp = 0\n",
    "#Cost: Test Data\n",
    "for samp in range(len(x_test)):\n",
    "    x_curr = x_test[samp]\n",
    "    z2 = np.matmul(x_curr,theta1.T)\n",
    "    a2 = n.sigmoid(z2)\n",
    "    a2 = np.hstack(([1], a2))\n",
    "    z3 = np.matmul(a2,theta2.T)\n",
    "    a3 = n.sigmoid(z3)\n",
    "\n",
    "    cost_temp += n.calc_cost(y_test[samp], a3, lambda_val, len(x_test))\n",
    "\n",
    "#Accumulate cost values and regularize to get Cost(J) \n",
    "term1 = (1/len(x_test))*cost_temp\n",
    "term2 = n.regularization(1, len(x_test))\n",
    "J = term1 + term2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ec935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32586142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
