{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fb3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize as sp\n",
    "import pandas as pd\n",
    "import math\n",
    "from random import sample, uniform\n",
    "\n",
    "class Neural_Network():\n",
    "    #calculates sigmoid function, params: z (variable), returns: sigmoid calculation\n",
    "    def sigmoid(self,z):\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    #calculates the sigmoidGradient (derivative), params: z (variable), returns: sigmoid gradient calculation \n",
    "    def sigmoidGradient(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "    \n",
    "    def randomly_initialize(self, num_in, num_out):\n",
    "        epsilon = 0.12\n",
    "        size = num_out*(num_in+1)\n",
    "        all_weights = np.zeros((size,))\n",
    "        for i in range(size):   \n",
    "            all_weights[i] = uniform(-epsilon, epsilon) \n",
    "        weights = np.reshape(all_weights,(num_out,num_in+1))\n",
    "        return weights\n",
    "    \n",
    "    #calculates the regularization term for the neural network, params: lambda (regularization constant),\n",
    "    #m (number of training samples), returns regularization value\n",
    "    def regularization(self, lamda, m):\n",
    "        lamda_val = lamda/(2.0*m)\n",
    "        theta1_sum = 0 \n",
    "        theta2_sum = 0\n",
    "        for j in range(len(self.Theta1)-1):\n",
    "            for k in range(self.Theta1[0].size-1):\n",
    "                theta1_sum += self.Theta1[j+1][k+1]*self.Theta1[j+1][k+1]\n",
    "        for j in range(len(self.Theta2)-1):\n",
    "            for k in range(self.Theta2[0].size-1):\n",
    "                theta2_sum += self.Theta2[j+1][k+1]*self.Theta2[j+1][k+1]\n",
    "        return lamda_val*(theta1_sum+theta2_sum)\n",
    "    \n",
    "    #calculates the cost for the neural network, params: y_vals (expected output values), hyp (calculated output values),\n",
    "    #m (number of training samples), returns cost between given sample and expected value  \n",
    "    def calc_cost(self, y_vals, hyp, lamda, m): #hyp and y are both 10x1 vectors \n",
    "        cost = 0\n",
    "        for k in range(len(y_vals)):\n",
    "            cost += -y_vals[k] * math.log(hyp[k]) - (1-y_vals[k])*math.log(1-hyp[k])\n",
    "        return cost\n",
    "    \n",
    "    #predicts the number that correlates to the input data, params: weights(an array that consists of 2 weight matricies),\n",
    "    #x_vals (array that consists of input values), returns prediction number (0-9) \n",
    "    def predict(self, weights, x_vals):\n",
    "            #x_vals = np.hstack(([1],x_vals))\n",
    "            weights1 = weights[0]\n",
    "            weights2 = weights[1]\n",
    "            z2 = np.matmul(x_vals,weights1.T)\n",
    "            a2 = self.sigmoid(z2)\n",
    "            a2 = np.hstack(([1], a2))\n",
    "            z3 = np.matmul(a2,weights2.T)\n",
    "            a3 = self.sigmoid(z3)\n",
    "            max_val = a3[0]\n",
    "            max_index = 0\n",
    "            print(a3)\n",
    "            for i in range(len(a3)):\n",
    "                if (a3[i] > max_val):\n",
    "                    max_val = a3[i]\n",
    "                    max_index = i\n",
    "            prediction = max_index+1\n",
    "            if prediction == 10:\n",
    "                prediction = 0\n",
    "            return prediction\n",
    "        \n",
    "    #performs forward and backward prop to get a final cost value, J, and 2 gradient weight matricies\n",
    "    #params: nn_params(array that consists of 2 weight matricies for layer 1 and 2 respectively), input_layer_size (number of input units),\n",
    "    #hidden_layer_size (number of hidden units), num_labels (number of output units), x (training samples), y (expected output values), lambda_reg (regularization constant)\n",
    "    #returns cost and an array of weight gradient vectors \n",
    "    def nnCostFunction(self, nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_reg):\n",
    "        self.Theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)],(hidden_layer_size, input_layer_size+1))\n",
    "        self.Theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "        \n",
    "        J = 0;\n",
    "        Theta1_grad = np.zeros_like(self.Theta1)\n",
    "        Theta2_grad = np.zeros_like(self.Theta2)\n",
    "       \n",
    "        #Forward and Back prop: \n",
    "\n",
    "        bigDelta1 = 0\n",
    "        bigDelta2 = 0\n",
    "        cost_temp = 0\n",
    "\n",
    "        # for each training example\n",
    "        for t in range(m):\n",
    "\n",
    "            ## step 1: perform forward pass\n",
    "            x = X[t]\n",
    "\n",
    "            #calculate z2 (linear combination) and a2 (activation for layer 2)\n",
    "            z2 = np.matmul(x,self.Theta1.T)\n",
    "            a2 = self.sigmoid(z2)\n",
    "\n",
    "            # add column of ones as bias unit to the second layer\n",
    "            a2 = np.hstack(([1], a2))\n",
    "            # calculate z3 (linear combination) and a3 (activation for layer 3 aka final hypothesis)\n",
    "            z3 = np.matmul(a2,self.Theta2.T)\n",
    "            a3 = self.sigmoid(z3)\n",
    "            \n",
    "            #Backpropogation: \n",
    "\n",
    "            #step 2: set delta 3\n",
    "            delta3 = np.zeros((num_labels))\n",
    "\n",
    "            #Get Error: subtract actual val in y from each hypothesized val in a3  \n",
    "            y_vals = np.zeros((num_labels))\n",
    "            for k in range(num_labels): #for each of the 10 labels subtract\n",
    "                y_k = y[t][k]\n",
    "                y_vals[k] = y_k\n",
    "                delta3[k] = a3[k] - y_k\n",
    "\n",
    "            #step 3: for layer 2 set delta2 = Theta2 Transpose * delta3 .* sigmoidGradient(z2) (= Chain Rule)\n",
    "            #Skip over the bias unit in layer 2: no gradient calculated for this value \n",
    "            delta2 = np.matmul(self.Theta2[:,1:].T, delta3) * self.sigmoidGradient(z2)\n",
    "\n",
    "            #step 4: accumulate gradient from this sample\n",
    "            bigDelta1 += np.outer(delta2, x)\n",
    "            bigDelta2 += np.outer(delta3, a2)\n",
    "            #Update the total cost given the cost from this sample\n",
    "            cost_temp += self.calc_cost(y_vals, a3, lambda_reg, m)\n",
    "            \n",
    "        #Accumulate cost values and regularize to get Cost(J) \n",
    "        term1 = (1/m)*cost_temp\n",
    "        term2 = self.regularization(lambda_reg, m)\n",
    "        J = term1 + term2\n",
    "        print(\"Cost: \" + str(J)) \n",
    "        \n",
    "        # step 5: obtain gradient for neural net cost function by dividing the accumulated gradients by m\n",
    "        Theta1_grad = bigDelta1 / m\n",
    "        Theta2_grad = bigDelta2 / m\n",
    "        \n",
    "\n",
    "        #Regularization\n",
    "        #only regularize for j >= 1, so skip the first column\n",
    "        Theta1_grad_unregularized = np.copy(Theta1_grad)\n",
    "        Theta2_grad_unregularized = np.copy(Theta2_grad)\n",
    "        Theta1_grad += (float(lambda_reg)/m)*self.Theta1\n",
    "        Theta2_grad += (float(lambda_reg)/m)*self.Theta2\n",
    "        Theta1_grad[:,0] = Theta1_grad_unregularized[:,0]\n",
    "        Theta2_grad[:,0] = Theta2_grad_unregularized[:,0]\n",
    "        flattened_grads = np.hstack((Theta1_grad.flatten(),Theta2_grad.flatten()))\n",
    "        \n",
    "        return J, flattened_grads\n",
    "        \n",
    "        \n",
    "#Read in data files\n",
    "#df = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Theta1.csv', header = None)\n",
    "#df2 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Theta2.csv', header = None)\n",
    "df3 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/X.csv', header = None)\n",
    "df4 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Y.csv', header = None)\n",
    "#Initialize layer sizes\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 100\n",
    "num_labels = 10\n",
    "#Set sizes for weight and data matricies\n",
    "theta1 = np.zeros([hidden_layer_size,input_layer_size+1])\n",
    "theta2 = np.zeros([num_labels,hidden_layer_size+1])\n",
    "x = np.zeros((len(df3),input_layer_size))\n",
    "x_sample = np.zeros((int(len(df3)*(0.7)), input_layer_size)) #take only 70% for training to leave 30% for testing\n",
    "y_vec = np.zeros((len(df4),))\n",
    "y_sample = np.zeros((int(len(df4)*0.7)),)\n",
    "random_indicies = sample(range(0,int(len(df3))),int(len(df3)*0.7)) \n",
    "\n",
    "n = Neural_Network()\n",
    "\n",
    "#create data and weight arrays\n",
    "theta1 = n.randomly_initialize(input_layer_size, hidden_layer_size)\n",
    "theta2 = n.randomly_initialize(hidden_layer_size, num_labels)\n",
    "\n",
    "index = 0\n",
    "while(index<len(x)):\n",
    "    x[index] = df3.iloc[index]\n",
    "    index+=1\n",
    "\n",
    "index = 0\n",
    "while (index<len(y_vec)):\n",
    "    y_vec[index] = df4.iloc[index]\n",
    "    index+=1\n",
    "    \n",
    "for index in range(len(random_indicies)):\n",
    "    sample_index = random_indicies[index] \n",
    "    x_sample[index] = x[sample_index] \n",
    "    y_sample[index] = y_vec[sample_index]\n",
    "x_test = np.zeros((int(len(df3)*0.3),input_layer_size))\n",
    "y_test = np.zeros((int(len(df4)*0.3),))\n",
    "#set test data\n",
    "test_indicies = np.zeros((int(len(df3)*0.3),))\n",
    "count = 0\n",
    "for ind in range(len(df3)):\n",
    "    if ind not in random_indicies:\n",
    "        test_indicies[count] = ind\n",
    "        count+=1\n",
    "for ii in range(len(test_indicies)):\n",
    "    test_index = int(test_indicies[ii])\n",
    "    x_test[ii] = x[test_index]\n",
    "    y_test[ii] = y_vec[test_index]\n",
    "    \n",
    "x = x_sample\n",
    "ones = np.ones((len(x_sample),1))\n",
    "test_ones = np.ones((len(x_test),1))\n",
    "x = np.hstack((ones, x)) \n",
    "x_test = np.hstack((test_ones,x_test))\n",
    "y_vec = y_sample\n",
    "    \n",
    "m = len(x)\n",
    "# set y to be a 2-D matrix with each column being a different sample and each row corresponding to a value 0-9\n",
    "y = np.zeros((m,num_labels))\n",
    "y_test_matrix = np.zeros((len(y_test),num_labels))\n",
    "# for every label, convert it into vector of 0s and a 1 in the appropriate position\n",
    "for i in range(m): #each row is new training sample\n",
    "    index = int(y_vec[i]-1)\n",
    "    y[i][index] = 1\n",
    "y_temp = y_test\n",
    "for j in range(int(len(y_test))):\n",
    "    index2 = int(y_temp[j]-1)\n",
    "    y_test_matrix[j][index2] = 1\n",
    "y_test = y_test_matrix\n",
    "nn_params = [theta1, theta2] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0badf271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.4648209964709733\n",
      "Cost: 0.4648209964709733\n",
      "Cost: 0.4648204812950939\n",
      "Cost: 0.46482038135777265\n",
      "Cost: 0.46482026187998027\n",
      "Cost: 0.4648200726694377\n",
      "Cost: 0.46481974674103554\n",
      "Cost: 0.464819534970033\n",
      "Cost: 0.46481927483838786\n",
      "Cost: 0.4648189959980751\n",
      "Cost: 0.46481896315309307\n",
      "Cost: 0.4648185956712864\n",
      "Cost: 0.4648177765426701\n",
      "Cost: 0.46482491794417957\n",
      "Cost: 0.4648177687640359\n",
      "Cost: 0.46481597507675254\n",
      "Cost: 0.46481310144240584\n",
      "Cost: 0.4648119807726956\n",
      "Cost: 0.4648131862988843\n",
      "Cost: 0.46481137982281245\n",
      "Cost: 0.4648107975612348\n",
      "Cost: 0.4648104958202931\n",
      "Cost: 0.46481019855748784\n",
      "Cost: 0.464813895467985\n",
      "Cost: 0.46481021425484126\n",
      "Cost: 0.4648097039615189\n",
      "Cost: 0.46480836542920406\n",
      "Cost: 0.4648080420804877\n",
      "Cost: 0.4648078476174239\n",
      "Cost: 0.46480770447202263\n",
      "Cost: 0.46480733572454\n",
      "Cost: 0.46480715089186464\n",
      "Cost: 0.4648065222294584\n",
      "Cost: 0.46480575842949295\n",
      "Cost: 0.46480427683834596\n",
      "Cost: 0.464800998827696\n",
      "Cost: 0.46479957473001815\n",
      "Cost: 0.4647983921853468\n",
      "Cost: 0.46479725133361494\n",
      "Cost: 0.46479646635262417\n",
      "Cost: 0.46479591944383136\n",
      "Cost: 0.4647945314250932\n",
      "Cost: 0.4647936933482548\n",
      "Cost: 0.464792204221126\n",
      "Cost: 0.46479103945163724\n",
      "Cost: 0.46478949190084573\n",
      "Cost: 0.46478807714032155\n",
      "Cost: 0.46478644127920865\n",
      "Cost: 0.4647852609580982\n",
      "Cost: 0.4647843423254956\n",
      "Cost: 0.4647831453193989\n",
      "Cost: 0.46478123597310383\n",
      "Cost: 0.4647797899960646\n",
      "Cost: 0.46477772544996676\n",
      "Cost: 0.46477579152544135\n",
      "Cost: 0.46477345296858896\n",
      "Cost: 0.46477149115877525\n",
      "Cost: 0.46477073538502534\n",
      "Cost: 0.4647696698540337\n",
      "Cost: 0.46476781902422193\n",
      "Cost: 0.4647654099990363\n",
      "Cost: 0.4647620417889993\n",
      "Cost: 0.4647553455003809\n",
      "Cost: 0.46474664873616234\n",
      "Cost: 0.4647406346064438\n",
      "Cost: 0.46473689790365297\n",
      "Cost: 0.4647292072597703\n",
      "Cost: 0.4647250785186027\n",
      "Cost: 0.4647215010907575\n",
      "Cost: 0.4647149397100382\n",
      "Cost: 0.46472737150348464\n",
      "Cost: 0.4647121821405256\n",
      "Cost: 0.46469670018257625\n",
      "Cost: 0.46469912642224\n",
      "Cost: 0.4646850290907052\n",
      "Cost: 0.46467571041170475\n",
      "Cost: 0.46466531809743683\n",
      "Cost: 0.46465905356346804\n",
      "Cost: 0.46465447072838806\n",
      "Cost: 0.4646492552369464\n",
      "Cost: 0.46464915377482474\n",
      "Cost: 0.4646450537431776\n",
      "Cost: 0.46464039250961287\n",
      "Cost: 0.46464042404191386\n",
      "Cost: 0.4646366739656901\n",
      "Cost: 0.4646344174198397\n",
      "Cost: 0.46463631857993587\n",
      "Cost: 0.4646330794318747\n",
      "Cost: 0.4646312901504335\n",
      "Cost: 0.4646309645290204\n",
      "Cost: 0.4646297559540703\n",
      "Cost: 0.46462837078788427\n",
      "Cost: 0.4646273667809407\n",
      "Cost: 0.4646268919242794\n",
      "Cost: 0.46462505178439595\n",
      "Cost: 0.4646225902811816\n",
      "Cost: 0.4646205594161531\n",
      "Cost: 0.4646236511927192\n",
      "Cost: 0.4646195960234409\n",
      "Cost: 0.4646173165468941\n",
      "Cost: 0.4646171331899961\n",
      "Cost: 0.46461543825594764\n",
      "Cost: 0.4646146255521287\n",
      "Cost: 0.4646158950205672\n",
      "Cost: 0.46461424490072767\n",
      "Cost: 0.4646136587723473\n",
      "Cost: 0.4646128834661865\n",
      "Cost: 0.46461195972734637\n",
      "Cost: 0.46461053658475276\n",
      "Cost: 0.46460921564084856\n",
      "Cost: 0.4646142800019873\n",
      "Cost: 0.4646089038912744\n",
      "Cost: 0.4646084109831504\n",
      "Cost: 0.4646076759217056\n",
      "Cost: 0.4646069381399154\n",
      "Cost: 0.46460545612103876\n",
      "Cost: 0.4646047479522984\n",
      "Cost: 0.46460396628093426\n",
      "Cost: 0.4646037555389969\n",
      "Cost: 0.46460328158880637\n",
      "Cost: 0.46460294107754607\n",
      "Cost: 0.4646019942390468\n",
      "Cost: 0.4646048563512686\n",
      "Cost: 0.4646017101810491\n",
      "Cost: 0.46460104857239226\n",
      "Cost: 0.464600827960336\n",
      "Cost: 0.46460031370938426\n",
      "Cost: 0.4646056557695614\n",
      "Cost: 0.4646003220396261\n",
      "Cost: 0.46459940406394906\n",
      "Cost: 0.4645974479729209\n",
      "Cost: 0.4645967456540382\n",
      "Cost: 0.46459613622332663\n",
      "Cost: 0.46459572177160813\n",
      "Cost: 0.4645948910539777\n",
      "Cost: 0.4645943907587845\n",
      "Cost: 0.4645942248862295\n",
      "Cost: 0.4645939381536298\n",
      "Cost: 0.4645937519918477\n",
      "Cost: 0.4645938096503829\n",
      "Cost: 0.4645936219058806\n",
      "Cost: 0.4645934965276587\n",
      "Cost: 0.4645931894163651\n",
      "Cost: 0.4645929202157569\n",
      "Cost: 0.4645922081950672\n",
      "Cost: 0.46459185708066125\n",
      "Cost: 0.4645911989825564\n",
      "Cost: 0.4645907865451304\n",
      "Cost: 0.4645904335373455\n",
      "Cost: 0.46458974139681275\n",
      "Cost: 0.46459055911668645\n",
      "Cost: 0.46458937519422294\n",
      "Cost: 0.4645886829651502\n",
      "Cost: 0.46458761706857443\n",
      "Cost: 0.4645864374259474\n",
      "Cost: 0.4645833628581723\n",
      "Cost: 0.46458219641333887\n",
      "Cost: 0.4645809161860679\n",
      "Cost: 0.4645803084687159\n",
      "Cost: 0.464580010049744\n",
      "Cost: 0.46457962632300887\n",
      "Cost: 0.4645785813225085\n",
      "Cost: 0.4645822351649759\n",
      "Cost: 0.46457830912016984\n",
      "Cost: 0.4645773861607338\n",
      "Cost: 0.46457629118342925\n",
      "Cost: 0.46457489563849597\n",
      "Cost: 0.4645742708453869\n",
      "Cost: 0.464572800577434\n",
      "Cost: 0.46457196304489795\n",
      "Cost: 0.46457121427050285\n",
      "Cost: 0.4645708851248431\n",
      "Cost: 0.46457056797808727\n",
      "Cost: 0.46456973729137213\n",
      "Cost: 0.4645694034119867\n",
      "Cost: 0.46456961213591896\n",
      "Cost: 0.46456919141292535\n",
      "Cost: 0.4645689343999788\n",
      "Cost: 0.4645685385929166\n",
      "Cost: 0.46456823515136336\n",
      "Cost: 0.4645677645384588\n",
      "Cost: 0.46456724136281013\n",
      "Cost: 0.46456713521002935\n",
      "Cost: 0.4645667893947602\n",
      "Cost: 0.46456647471725687\n",
      "Cost: 0.46456649776318015\n",
      "Cost: 0.4645662303661932\n",
      "Cost: 0.4645659031841315\n",
      "Cost: 0.4645652310638364\n",
      "Cost: 0.46456472287254946\n",
      "Cost: 0.46456328917586365\n",
      "Cost: 0.4645671437138525\n",
      "Cost: 0.4645628142077429\n",
      "Cost: 0.46456164304684244\n",
      "Cost: 0.464561293073828\n",
      "Cost: 0.4645605923265421\n",
      "Cost: 0.46455937634740196\n",
      "Cost: 0.4645578222189873\n",
      "Cost: 0.4645566612146\n",
      "Cost: 0.4645530222683326\n",
      "Cost: 0.4645545404731227\n",
      "Cost: 0.4645505236370825\n",
      "Cost: 0.46454622175349974\n",
      "Cost: 0.4645511714440762\n",
      "Cost: 0.4645439196381347\n",
      "Cost: 0.46454139830707597\n",
      "Cost: 0.4645357895736031\n",
      "Cost: 0.4645330154452477\n",
      "Cost: 0.4645270264213425\n",
      "Cost: 0.4645214664934434\n",
      "Cost: 0.4645051421537383\n",
      "Cost: 0.46449861621101796\n",
      "Cost: 0.46447125941271594\n",
      "Cost: 0.4644526173131033\n",
      "Cost: 0.4644182522667132\n",
      "Cost: 0.46440859222450726\n",
      "Cost: 0.46439865855441353\n",
      "Cost: 0.46439362003127405\n",
      "Cost: 0.464384581226541\n",
      "Cost: 0.4643762163827943\n",
      "Cost: 0.46436675579588205\n",
      "Cost: 0.46436458625420274\n",
      "Cost: 0.46436408338721813\n",
      "Cost: 0.4643634522440316\n",
      "Cost: 0.4643625743088582\n",
      "Cost: 0.4643619835504287\n",
      "Cost: 0.46436126443117276\n",
      "Cost: 0.46436095082268924\n",
      "Cost: 0.4643602797267392\n",
      "Cost: 0.4643600942511554\n",
      "Cost: 0.4643595270555896\n",
      "Cost: 0.46435920130327657\n",
      "Cost: 0.4643583279876159\n",
      "Cost: 0.46435604435907196\n",
      "Cost: 0.4643546609376125\n",
      "Cost: 0.46435341905007155\n",
      "Cost: 0.46435299797447105\n",
      "Cost: 0.4643517542733969\n",
      "Cost: 0.46435383083946513\n",
      "Cost: 0.46435119132200986\n",
      "Cost: 0.4643492654397052\n",
      "Cost: 0.46434604675164126\n",
      "Cost: 0.464343893391435\n",
      "Cost: 0.4643408827182489\n",
      "Cost: 0.46433881669567856\n",
      "Cost: 0.4643373376316664\n",
      "Cost: 0.4643362287962234\n",
      "Cost: 0.4643331614863747\n",
      "Cost: 0.46434278407217255\n",
      "Cost: 0.46433227248919895\n",
      "Cost: 0.4643283493229986\n",
      "Cost: 0.4643213567076987\n",
      "Cost: 0.46431784753855476\n",
      "Cost: 0.4643157875113814\n",
      "Cost: 0.46431451725879114\n",
      "Cost: 0.46431173705187767\n",
      "Cost: 0.4643071979288844\n",
      "Cost: 0.4643044882187017\n",
      "Cost: 0.46429808324256583\n",
      "Cost: 0.46429455814601805\n",
      "Cost: 0.46428880857750776\n",
      "Cost: 0.46428497491356685\n",
      "Cost: 0.4642822979801733\n",
      "Cost: 0.4642753707516157\n",
      "Cost: 0.4642694595050646\n",
      "Cost: 0.4642597580178597\n",
      "Cost: 0.46425377298171233\n",
      "Cost: 0.4642470061232302\n",
      "Cost: 0.46424202622833244\n",
      "Cost: 0.4642380559920608\n",
      "Cost: 0.464229465499324\n",
      "Cost: 0.4642258661215074\n",
      "Cost: 0.4642217722479338\n",
      "Cost: 0.46421975510457314\n",
      "Cost: 0.4642244424809855\n",
      "Cost: 0.4642190096296378\n",
      "Cost: 0.46421483981822237\n",
      "Cost: 0.4642103139639464\n",
      "Cost: 0.46420724055909757\n",
      "Cost: 0.464203725679237\n",
      "Cost: 0.46420097120020454\n",
      "Cost: 0.46419818145932024\n",
      "Cost: 0.4641956697743795\n",
      "Cost: 0.4641922335984429\n",
      "Cost: 0.46419031917278486\n",
      "Cost: 0.464188968722269\n",
      "Cost: 0.46418701290326037\n",
      "Cost: 0.464183583233409\n",
      "Cost: 0.46418107738011677\n",
      "Cost: 0.4641781011807322\n",
      "Cost: 0.4641760652563377\n",
      "Cost: 0.46417428930800114\n",
      "Cost: 0.4641726650346785\n",
      "Cost: 0.46417692299592306\n",
      "Cost: 0.4641721164194407\n",
      "Cost: 0.4641700010188141\n",
      "Cost: 0.46416915294803807\n",
      "Cost: 0.4641684103557886\n",
      "Cost: 0.4641675119579003\n",
      "Cost: 0.4641669232853731\n",
      "Cost: 0.4641662567932239\n",
      "Cost: 0.46416581761028686\n",
      "Cost: 0.4641656830074964\n",
      "Cost: 0.46416542431650587\n",
      "Cost: 0.4641652433081124\n",
      "Cost: 0.46416491424724365\n",
      "Cost: 0.46416447177284553\n",
      "Cost: 0.4641629964936266\n",
      "Cost: 0.4641613664784038\n",
      "Cost: 0.46415975996826125\n",
      "Cost: 0.46415816667329735\n",
      "Cost: 0.46415703844524137\n",
      "Cost: 0.46415604757728046\n",
      "Cost: 0.4641550116907616\n",
      "Cost: 0.4641526824521342\n",
      "Cost: 0.4641513799226081\n",
      "Cost: 0.46414791022025786\n",
      "Cost: 0.4641471044881024\n",
      "Cost: 0.4641459996228265\n",
      "Cost: 0.4641452602885271\n",
      "Cost: 0.4641436805993122\n",
      "Cost: 0.4641416844968878\n",
      "Cost: 0.46413743127953466\n",
      "Cost: 0.46413534331709816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.46413173901783256\n",
      "Cost: 0.46413012527933917\n",
      "Cost: 0.4641292022601331\n",
      "Cost: 0.46412783080079106\n",
      "Cost: 0.464124823890409\n",
      "Cost: 0.4641234880584252\n",
      "Cost: 0.4641221481094201\n",
      "Cost: 0.4641213738678418\n",
      "Cost: 0.46412069560613306\n",
      "Cost: 0.46411955119667125\n",
      "Cost: 0.4641161885169263\n",
      "Cost: 0.4641147241620037\n",
      "Cost: 0.4641117840704537\n",
      "Cost: 0.4641092847620303\n",
      "Cost: 0.46410619430829814\n",
      "Cost: 0.46410253218661296\n",
      "Cost: 0.46409982826362056\n",
      "Cost: 0.4640960030596712\n",
      "Cost: 0.4640928971686169\n",
      "Cost: 0.46409017323484564\n",
      "Cost: 0.4640871694755712\n",
      "Cost: 0.46408318167984497\n",
      "Cost: 0.46408058944084074\n",
      "Cost: 0.46407732042471705\n",
      "Cost: 0.4640738167059641\n",
      "Cost: 0.4640705208471281\n",
      "Cost: 0.46406744930004945\n",
      "Cost: 0.4640615927868462\n",
      "Cost: 0.4640573425779497\n",
      "Cost: 0.4640540670415261\n",
      "Cost: 0.4640514179982146\n",
      "Cost: 0.46404909792166954\n",
      "Cost: 0.4640465420972032\n",
      "Cost: 0.4640428165483459\n",
      "Cost: 0.46403901158904015\n",
      "Cost: 0.4640326823723799\n",
      "Cost: 0.4640281799418833\n",
      "Cost: 0.46402553885681896\n",
      "Cost: 0.4640221242355773\n",
      "Cost: 0.46401694497381996\n",
      "Cost: 0.4640145207360895\n",
      "Cost: 0.4640127283316332\n",
      "Cost: 0.46401007801044175\n",
      "Cost: 0.4640074257772199\n",
      "Cost: 0.4640039534766026\n",
      "Cost: 0.46400292735744\n",
      "Cost: 0.46400083996416563\n",
      "Cost: 0.46399528416331604\n",
      "Cost: 0.46399340888881446\n",
      "Cost: 0.46399022739006773\n",
      "Cost: 0.4639844459959084\n",
      "Cost: 0.46398029076140207\n",
      "Cost: 0.4639733218784634\n",
      "Cost: 0.46396951611729353\n",
      "Cost: 0.4639698221326002\n",
      "Cost: 0.4639665664475792\n",
      "Cost: 0.4639649220960098\n",
      "Cost: 0.4639651082888649\n",
      "Cost: 0.46396366304685366\n",
      "Cost: 0.46396224318018786\n",
      "Cost: 0.46395902233822517\n",
      "Cost: 0.4639571165334632\n",
      "Cost: 0.46395479401162537\n",
      "Cost: 0.4639537882461908\n",
      "Cost: 0.46395820206456573\n",
      "Cost: 0.4639535867194807\n",
      "Cost: 0.46395182450959405\n",
      "Cost: 0.4639560707013161\n",
      "Cost: 0.46395118934996327\n",
      "Cost: 0.46395047644840615\n",
      "Cost: 0.46394998569910195\n",
      "Cost: 0.46394902829167517\n",
      "Cost: 0.46395025454496497\n",
      "Cost: 0.46394854112991757\n",
      "Cost: 0.46394657159524294\n",
      "Cost: 0.4639461738962634\n",
      "Cost: 0.46394564234524194\n",
      "Cost: 0.46394520229844655\n",
      "Cost: 0.46394480495551504\n",
      "Cost: 0.4639436323387037\n",
      "Cost: 0.46394561479166263\n",
      "Cost: 0.46394310486563883\n",
      "Cost: 0.4639419148790088\n",
      "Cost: 0.463939742875469\n",
      "Cost: 0.46393789595880647\n",
      "Cost: 0.46393695474364643\n",
      "Cost: 0.4639359848960195\n",
      "Cost: 0.46394128196875417\n",
      "Cost: 0.4639358491783294\n",
      "Cost: 0.46393381458640204\n",
      "Cost: 0.4639291042806606\n",
      "Cost: 0.4639269811640791\n",
      "Cost: 0.4639251728639562\n",
      "Cost: 0.46392367218335656\n",
      "Cost: 0.46392149409619793\n",
      "Cost: 0.46392058216047005\n",
      "Cost: 0.4639201882317663\n",
      "Cost: 0.46391997970612386\n",
      "Cost: 0.46392094607135226\n",
      "Cost: 0.4639199410151344\n",
      "Cost: 0.4639194342228527\n",
      "Cost: 0.4639182327073724\n",
      "Cost: 0.46391792574304724\n",
      "Cost: 0.46391719228850803\n",
      "Cost: 0.4639168742054218\n",
      "Cost: 0.46391620362303515\n",
      "Cost: 0.46391588491548064\n",
      "Cost: 0.46391556809565027\n",
      "Cost: 0.46391523315271743\n",
      "Cost: 0.4639152624914781\n",
      "Cost: 0.4639149748490612\n",
      "Cost: 0.4639139245249533\n",
      "Cost: 0.4639129024356997\n",
      "Cost: 0.46391183741932895\n",
      "Cost: 0.46390942429825077\n",
      "Cost: 0.46390760381126483\n",
      "Cost: 0.4639048996884492\n",
      "Cost: 0.4639032177518916\n",
      "Cost: 0.46390500525045875\n",
      "Cost: 0.4639022923202417\n",
      "Cost: 0.46390078228502085\n",
      "Cost: 0.46390021348588584\n",
      "Cost: 0.46389946990464104\n",
      "Cost: 0.4638987501747348\n",
      "Cost: 0.4638975287913392\n",
      "Cost: 0.4638959159822754\n",
      "Cost: 0.4638952532263616\n",
      "Cost: 0.4638949940761512\n",
      "Cost: 0.4638946362257758\n",
      "Cost: 0.46389431443837215\n",
      "Cost: 0.4638939184431649\n",
      "Cost: 0.4638943119285452\n",
      "Cost: 0.46389369529340163\n",
      "Cost: 0.4638926443395733\n",
      "Cost: 0.4638923215243108\n",
      "Cost: 0.4638916978617985\n",
      "Cost: 0.46389117060230084\n",
      "Cost: 0.4638908461693949\n",
      "Cost: 0.46389035768763687\n",
      "Cost: 0.4638894252378375\n",
      "Cost: 0.4638888492804835\n",
      "Cost: 0.46388928244709715\n",
      "Cost: 0.46388849712464836\n",
      "Cost: 0.4638880061147742\n",
      "Cost: 0.46388736321696167\n",
      "Cost: 0.4638866694689505\n",
      "Cost: 0.4638861235408931\n",
      "Cost: 0.4638840763197498\n",
      "Cost: 0.46387969764570547\n",
      "Cost: 0.46387392681246964\n",
      "Cost: 0.46387059938488373\n",
      "Cost: 0.4638680172262255\n",
      "Cost: 0.4638641526852255\n",
      "Cost: 0.4638608998770125\n",
      "Cost: 0.46385830967173963\n",
      "Cost: 0.4638553618643253\n",
      "Cost: 0.46385279051813244\n",
      "Cost: 0.4638507269188474\n",
      "Cost: 0.4638492249627896\n",
      "Cost: 0.4638480160813756\n",
      "Cost: 0.46384724432514934\n",
      "Cost: 0.46384621847616325\n",
      "Cost: 0.4638445728368385\n",
      "Cost: 0.46384317225926897\n",
      "Cost: 0.46384162753972535\n",
      "Cost: 0.4638401868838353\n",
      "Cost: 0.46383912529707966\n",
      "Cost: 0.46383816463136596\n",
      "Cost: 0.463837095859907\n",
      "Cost: 0.46383604671762263\n",
      "Cost: 0.4638351107250581\n",
      "Cost: 0.46383413277304647\n",
      "Cost: 0.4638322408565958\n",
      "Cost: 0.4638303490299609\n",
      "Cost: 0.4638280980184081\n",
      "Cost: 0.4638261994871478\n",
      "Cost: 0.4638237818461584\n",
      "Cost: 0.46382210425644216\n",
      "Cost: 0.46382083519703987\n",
      "Cost: 0.4638189042441305\n",
      "Cost: 0.4638150133267718\n",
      "Cost: 0.46381201332012817\n",
      "Cost: 0.463807824275108\n",
      "Cost: 0.4638030959530959\n",
      "Cost: 0.4637954063756614\n",
      "Cost: 0.46378944328133764\n",
      "Cost: 0.46378326554952376\n",
      "Cost: 0.46377709161225866\n",
      "Cost: 0.46377139780388393\n",
      "Cost: 0.463766160919481\n",
      "Cost: 0.4637579875956208\n",
      "Cost: 0.4637483656007502\n",
      "Cost: 0.4637353986837397\n",
      "Cost: 0.4637244917804395\n",
      "Cost: 0.46371026558289746\n",
      "Cost: 0.4637007268355153\n",
      "Cost: 0.46369503316834915\n",
      "Cost: 0.4636894730466642\n",
      "Cost: 0.4636836557043841\n",
      "Cost: 0.4636790189868716\n",
      "Cost: 0.4636744307188122\n",
      "Cost: 0.46366947325482333\n",
      "Cost: 0.463663256152363\n",
      "Cost: 0.46365780723071853\n",
      "Cost: 0.4636515957357701\n",
      "Cost: 0.4636476870497799\n",
      "Cost: 0.46364534781043765\n",
      "Cost: 0.4636428785692303\n",
      "Cost: 0.4636392858551788\n",
      "Cost: 0.46363611948601613\n",
      "Cost: 0.4636324524585801\n",
      "Cost: 0.4636284118535221\n",
      "Cost: 0.46362245848948935\n",
      "Cost: 0.4636183930438259\n",
      "Cost: 0.46361483340942866\n",
      "Cost: 0.4636108875446793\n",
      "Cost: 0.46360722145470523\n",
      "Cost: 0.46360512108510227\n",
      "Cost: 0.463602519762965\n",
      "Cost: 0.4635988879229886\n",
      "Cost: 0.4635930332194043\n",
      "Cost: 0.46358859894644067\n",
      "Cost: 0.4635834824099889\n",
      "Cost: 0.4635789049375092\n",
      "Cost: 0.46357550111891344\n",
      "Cost: 0.4635723454252538\n",
      "Cost: 0.4635683077688332\n",
      "Cost: 0.46356461959885753\n",
      "Cost: 0.4635613764190971\n",
      "Cost: 0.46355767282711924\n",
      "Cost: 0.4635504371481808\n",
      "Cost: 0.46354509405029914\n",
      "Cost: 0.4635414829738523\n",
      "Cost: 0.46353857077301686\n",
      "Cost: 0.46353619886761255\n",
      "Cost: 0.4635334284932455\n",
      "Cost: 0.4635297991455464\n",
      "Cost: 0.4635257195875042\n",
      "Cost: 0.4635211362135422\n",
      "Cost: 0.4635162053233124\n",
      "Cost: 0.4635094574806471\n",
      "Cost: 0.46350359966164467\n",
      "Cost: 0.4634991222393032\n",
      "Cost: 0.46349424207685674\n",
      "Cost: 0.4634903682981988\n",
      "Cost: 0.4634871622828614\n",
      "Cost: 0.4634858423085302\n",
      "Cost: 0.463483941345488\n",
      "Cost: 0.4634810101581255\n",
      "Cost: 0.4634791024074901\n",
      "Cost: 0.4634775509522356\n",
      "Cost: 0.46347569769057007\n",
      "Cost: 0.46347359585361597\n",
      "Cost: 0.4634721547990915\n",
      "Cost: 0.4634707867845619\n",
      "Cost: 0.4634692623834704\n",
      "Cost: 0.46346788442817155\n",
      "Cost: 0.46346646528902147\n",
      "Cost: 0.46346337355750666\n",
      "Cost: 0.46346061996683996\n",
      "Cost: 0.4634573395894902\n",
      "Cost: 0.46345516546890575\n",
      "Cost: 0.46345216363951924\n",
      "Cost: 0.4634495970124312\n",
      "Cost: 0.46344768426806215\n",
      "Cost: 0.4634457154446364\n",
      "Cost: 0.46344315825848537\n",
      "Cost: 0.4634409185155399\n",
      "Cost: 0.4634377185344826\n",
      "Cost: 0.4634342486049077\n",
      "Cost: 0.46342871404339636\n",
      "Cost: 0.46342451585009364\n",
      "Cost: 0.4634187625207097\n",
      "Cost: 0.46341532818988784\n",
      "Cost: 0.4634133895053248\n",
      "Cost: 0.46341117285599864\n",
      "Cost: 0.46340744595195854\n",
      "Cost: 0.46340413564498534\n",
      "Cost: 0.46339892968864327\n",
      "Cost: 0.46339407595097964\n",
      "Cost: 0.4633889878324836\n",
      "Cost: 0.46338578990629975\n",
      "Cost: 0.4633827640028407\n",
      "Cost: 0.46338015574255387\n",
      "Cost: 0.4633779252194472\n",
      "Cost: 0.4633761526407234\n",
      "Cost: 0.46337355857433316\n",
      "Cost: 0.46337057820133415\n",
      "Cost: 0.46336654892488227\n",
      "Cost: 0.4633631957306892\n",
      "Cost: 0.4633591759475104\n",
      "Cost: 0.4633570512282344\n",
      "Cost: 0.4633559849866942\n",
      "Cost: 0.46335438820165775\n",
      "Cost: 0.46335168994023546\n",
      "Cost: 0.46334972701442584\n",
      "Cost: 0.4633477437960058\n",
      "Cost: 0.4633454947945128\n",
      "Cost: 0.4633436287456544\n",
      "Cost: 0.46334153195712624\n",
      "Cost: 0.46334693292802653\n",
      "Cost: 0.4633408143579271\n",
      "Cost: 0.4633370455935141\n",
      "Cost: 0.4633359909889657\n",
      "Cost: 0.46333509628438\n",
      "Cost: 0.4633338536173464\n",
      "Cost: 0.4633325059949889\n",
      "Cost: 0.4633307859154817\n",
      "Cost: 0.46332980656074296\n",
      "Cost: 0.4633307765845269\n",
      "Cost: 0.46332925421977056\n",
      "Cost: 0.46332849140956944\n",
      "Cost: 0.4633272797604127\n",
      "Cost: 0.4633262265425295\n",
      "Cost: 0.46332584830107365\n",
      "Cost: 0.46332525986871115\n",
      "Cost: 0.4633243411307588\n",
      "Cost: 0.4633234069284513\n",
      "Cost: 0.46332283155777\n",
      "Cost: 0.4633237582500451\n",
      "Cost: 0.46332256615642586\n",
      "Cost: 0.4633215986187693\n",
      "Cost: 0.4633222601766075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.4633209929418103\n",
      "Cost: 0.46332047788630937\n",
      "Cost: 0.4633197357218736\n",
      "Cost: 0.46331905927106476\n",
      "Cost: 0.46331940747520534\n",
      "Cost: 0.46331861091461946\n",
      "Cost: 0.46331816372614315\n",
      "Cost: 0.46331751925896925\n",
      "Cost: 0.4633171440636828\n",
      "Cost: 0.46331670977479467\n",
      "Cost: 0.4633161131377316\n",
      "Cost: 0.4633151675799694\n",
      "Cost: 0.4633131076206277\n",
      "Cost: 0.46330903632322973\n",
      "Cost: 0.46330730428256833\n",
      "Cost: 0.4633084364621321\n",
      "Cost: 0.4633062116249965\n",
      "Cost: 0.46330486204761034\n",
      "Cost: 0.46330251887221774\n",
      "Cost: 0.46329981857232294\n",
      "Cost: 0.463293942145903\n",
      "Cost: 0.46328941891933473\n",
      "Cost: 0.4632968808083803\n",
      "Cost: 0.46328736129048653\n",
      "Cost: 0.46328593843611837\n",
      "Cost: 0.4632834767764721\n",
      "Cost: 0.4632808523696458\n",
      "Cost: 0.463278069648016\n",
      "Cost: 0.46327278940327843\n",
      "Cost: 0.4632730304211346\n",
      "Cost: 0.46326866505725206\n",
      "Cost: 0.4632617767137316\n",
      "Cost: 0.4632607554590283\n",
      "Cost: 0.46326040139930924\n",
      "Cost: 0.46326000286353763\n",
      "Cost: 0.46325898194482523\n",
      "Cost: 0.46325852354358144\n",
      "Cost: 0.46325742214539023\n",
      "Cost: 0.4632569457717053\n",
      "Cost: 0.4632558161347021\n",
      "Cost: 0.4632550404951287\n",
      "Cost: 0.4632531835117896\n",
      "Cost: 0.4632517800553344\n",
      "Cost: 0.46324816061717927\n",
      "Cost: 0.46324730602461195\n",
      "Cost: 0.46324606817457975\n",
      "Cost: 0.4632453738966746\n",
      "Cost: 0.46324404389173046\n",
      "Cost: 0.46324324958962715\n",
      "Cost: 0.46324278206014424\n",
      "Cost: 0.4632425575239402\n",
      "Cost: 0.46324189861047427\n",
      "Cost: 0.46324165231426717\n",
      "Cost: 0.46324083734609\n",
      "Cost: 0.46324002454517677\n",
      "Cost: 0.4632390047465795\n",
      "Cost: 0.463236344497458\n",
      "Cost: 0.4632349020450076\n",
      "Cost: 0.46323352829696757\n",
      "Cost: 0.4632328235930072\n",
      "Cost: 0.4632310696820257\n",
      "Cost: 0.46322983709910115\n",
      "Cost: 0.46322766917337344\n",
      "Cost: 0.46322672567837664\n",
      "Cost: 0.46322544600473947\n",
      "Cost: 0.463224657050644\n",
      "Cost: 0.46322372726849415\n",
      "Cost: 0.4632230971440973\n",
      "Cost: 0.463221356977086\n",
      "Cost: 0.463220883683171\n",
      "Cost: 0.4632195234955935\n",
      "Cost: 0.46321661997273184\n",
      "Cost: 0.46321472398385455\n",
      "Cost: 0.4632127974767597\n",
      "Cost: 0.4632120953223404\n",
      "Cost: 0.4632111559406611\n",
      "Cost: 0.4632097004873183\n",
      "Cost: 0.4632055124315858\n",
      "Cost: 0.46320403415655587\n",
      "Cost: 0.46320260937735896\n",
      "Cost: 0.46320242063222294\n",
      "Cost: 0.46320141164486717\n",
      "Cost: 0.46319977137606566\n",
      "Cost: 0.4631967828428346\n",
      "Cost: 0.4631944517615596\n",
      "Cost: 0.4631905852092083\n",
      "Cost: 0.46318753715551897\n",
      "Cost: 0.463187460172958\n",
      "Cost: 0.463185076345038\n",
      "Cost: 0.46318409244362535\n",
      "Cost: 0.46318304326969706\n",
      "Cost: 0.4631816192269498\n",
      "Cost: 0.46318044833276556\n",
      "Cost: 0.4631773398635479\n",
      "Cost: 0.4631813470195678\n",
      "Cost: 0.463175751509599\n",
      "Cost: 0.4631746689094711\n",
      "Cost: 0.46317514272495947\n",
      "Cost: 0.46317393086366265\n",
      "Cost: 0.46317332340879364\n",
      "Cost: 0.46317241764963124\n",
      "Cost: 0.46317192492128934\n",
      "Cost: 0.46317143741112515\n",
      "Cost: 0.46317102496901114\n",
      "Cost: 0.46317211076699827\n",
      "Cost: 0.46317088609269597\n",
      "Cost: 0.4631705658289954\n",
      "Cost: 0.4631698988209328\n",
      "Cost: 0.4631694972639743\n",
      "Cost: 0.463168988174265\n",
      "Cost: 0.46316873932562674\n",
      "Cost: 0.46316920452398935\n",
      "Cost: 0.4631686338266683\n",
      "Cost: 0.4631684119386902\n",
      "Cost: 0.4631686087143256\n",
      "Cost: 0.4631682823477581\n",
      "Cost: 0.4631681257709665\n",
      "Cost: 0.4631677660276988\n",
      "Cost: 0.4631676630600692\n",
      "Cost: 0.46316751337312745\n",
      "Cost: 0.463167385060568\n",
      "Cost: 0.46316705115611173\n",
      "Cost: 0.46316858566550834\n",
      "Cost: 0.4631669881325807\n",
      "Cost: 0.4631665679827573\n",
      "Cost: 0.4631657084619085\n",
      "Cost: 0.4631651491523153\n",
      "Cost: 0.4631640680490636\n",
      "Cost: 0.4631633458799818\n",
      "Cost: 0.4631619595836657\n",
      "Cost: 0.4631609638890051\n",
      "Cost: 0.46315975527156583\n",
      "Cost: 0.4631590800827554\n",
      "Cost: 0.4631598484915568\n",
      "Cost: 0.4631587222293734\n",
      "Cost: 0.4631579368063704\n",
      "Cost: 0.46315809783093786\n",
      "Cost: 0.4631573552339485\n",
      "Cost: 0.4631569534678631\n",
      "Cost: 0.4631565294549501\n",
      "Cost: 0.4631561213924903\n",
      "Cost: 0.46315706666623285\n",
      "Cost: 0.4631559702403048\n",
      "Cost: 0.46315550430921953\n",
      "Cost: 0.4631553182663736\n",
      "Cost: 0.4631551503512188\n",
      "Cost: 0.46315502491349686\n",
      "Cost: 0.4631548616385941\n",
      "Cost: 0.4631544777510822\n",
      "Cost: 0.463154272630504\n",
      "Cost: 0.46315390688323865\n",
      "Cost: 0.4631537122044642\n",
      "Cost: 0.4631532840534116\n",
      "Cost: 0.463152774034323\n",
      "Cost: 0.4631517559942424\n",
      "Cost: 0.46315120098059326\n",
      "Cost: 0.4631500486116696\n",
      "Cost: 0.46314967983500943\n",
      "Cost: 0.46314915315386573\n",
      "Cost: 0.4631489574608274\n",
      "Cost: 0.46314875480838225\n",
      "Cost: 0.46314851533127116\n",
      "Cost: 0.4631478791499163\n",
      "Cost: 0.4631475720405799\n",
      "Cost: 0.4631471727522975\n",
      "Cost: 0.46314674583958204\n",
      "Cost: 0.4631453817727423\n",
      "Cost: 0.46314432234802305\n",
      "Cost: 0.4631411052292245\n",
      "Cost: 0.46313335260222693\n",
      "Cost: 0.4631301080238216\n",
      "Cost: 0.4631270101376471\n",
      "Cost: 0.4631244571540902\n",
      "Cost: 0.46312108627907683\n",
      "Cost: 0.46311848842464765\n",
      "Cost: 0.4631154575747046\n",
      "Cost: 0.46311210426616\n",
      "Cost: 0.46310696674246377\n",
      "Cost: 0.46310409045975975\n",
      "Cost: 0.46310129830285635\n",
      "Cost: 0.4630982365184636\n",
      "Cost: 0.46309357292034803\n",
      "Cost: 0.4630894848912128\n",
      "Cost: 0.46308011994467\n",
      "Cost: 0.46307498972390854\n",
      "Cost: 0.46306994386359995\n",
      "Cost: 0.4630653651834573\n",
      "Cost: 0.46305990527855123\n",
      "Cost: 0.46305507734002793\n",
      "Cost: 0.463050145103945\n",
      "Cost: 0.46304536901794624\n",
      "Cost: 0.4630417076308019\n",
      "Cost: 0.4630387190541818\n",
      "Cost: 0.463035025046732\n",
      "Cost: 0.46303178218183183\n",
      "Cost: 0.46302765131530854\n",
      "Cost: 0.46302484838763086\n",
      "Cost: 0.463023048271611\n",
      "Cost: 0.4630209698664728\n",
      "Cost: 0.4630180728445342\n",
      "Cost: 0.46301618742940415\n",
      "Cost: 0.4630149649192539\n",
      "Cost: 0.46301376989066934\n",
      "Cost: 0.4630116280966522\n",
      "Cost: 0.4630098499956651\n",
      "Cost: 0.46300893081228206\n",
      "Cost: 0.4630079657858597\n",
      "Cost: 0.46300616284888096\n",
      "Cost: 0.46300379079464987\n",
      "Cost: 0.46300896545441295\n",
      "Cost: 0.4630028749707764\n",
      "Cost: 0.46300191822354525\n",
      "Cost: 0.46300030724585517\n",
      "Cost: 0.4629993383302441\n",
      "Cost: 0.4629979518754872\n",
      "Cost: 0.4629965815072864\n",
      "Cost: 0.4629957524656282\n",
      "Cost: 0.46299458278295935\n",
      "Cost: 0.4629958821249129\n",
      "Cost: 0.4629939593627795\n",
      "Cost: 0.4629924256140673\n",
      "Cost: 0.46299248004219395\n",
      "Cost: 0.4629912153139634\n",
      "Cost: 0.46299018544293224\n",
      "Cost: 0.4629891918427138\n",
      "Cost: 0.4629887236940915\n",
      "Cost: 0.46299045451933996\n",
      "Cost: 0.46298860869098846\n",
      "Cost: 0.4629881109279069\n",
      "Cost: 0.4629887213438004\n",
      "Cost: 0.46298785140794785\n",
      "Cost: 0.46298767960221576\n",
      "Cost: 0.46298774691099565\n",
      "Cost: 0.46298756150288206\n",
      "Cost: 0.46298747766376136\n",
      "Cost: 0.46298722738395703\n",
      "Cost: 0.46298758760340997\n",
      "Cost: 0.4629871065457597\n",
      "Cost: 0.46298696028394715\n",
      "Cost: 0.4629868925084885\n",
      "Cost: 0.4629868070060578\n",
      "Cost: 0.4629878010327385\n",
      "Cost: 0.46298681390776203\n",
      "Cost: 0.462986679313379\n",
      "Cost: 0.46298637222532263\n",
      "Cost: 0.4629862418829398\n",
      "Cost: 0.4629859328034645\n",
      "Cost: 0.4629858177994979\n",
      "Cost: 0.4629858816687469\n",
      "Cost: 0.4629857430315504\n",
      "Cost: 0.462985671478114\n",
      "Cost: 0.4629855380169847\n",
      "Cost: 0.46298548197897993\n",
      "Cost: 0.46298540916007636\n",
      "Cost: 0.46298529748314776\n",
      "Cost: 0.4629849495497441\n",
      "Cost: 0.4629851380938063\n",
      "Cost: 0.4629848865355586\n",
      "Cost: 0.4629844353960423\n",
      "Cost: 0.4629915214904404\n",
      "Cost: 0.4629845294992203\n",
      "Cost: 0.46298350387177384\n",
      "Cost: 0.4629827011247102\n",
      "Cost: 0.4629820049546257\n",
      "Cost: 0.4629812198175468\n",
      "Cost: 0.4629806180990848\n",
      "Cost: 0.4629803375909555\n",
      "Cost: 0.4629802691606124\n",
      "Cost: 0.4629802109001095\n",
      "Cost: 0.4629800691010104\n",
      "Cost: 0.4629798575582534\n",
      "Cost: 0.46297936232361214\n",
      "Cost: 0.46297913052670503\n",
      "Cost: 0.46297888686478356\n",
      "Cost: 0.4629786595285468\n",
      "Cost: 0.4629785710331042\n",
      "Cost: 0.46297848981792067\n",
      "Cost: 0.4629783743757556\n",
      "Cost: 0.4629780519924334\n",
      "Cost: 0.4629789918952878\n",
      "Cost: 0.4629779523035502\n",
      "Cost: 0.4629776585484591\n",
      "Cost: 0.46297729714036806\n",
      "Cost: 0.46297671125760376\n",
      "Cost: 0.46297629883944225\n",
      "Cost: 0.46297530315604385\n",
      "Cost: 0.4629792042289272\n",
      "Cost: 0.46297507385139197\n",
      "Cost: 0.46297449868621265\n",
      "Cost: 0.46297412313273345\n",
      "Cost: 0.46297384859990853\n",
      "Cost: 0.46297347472885\n",
      "Cost: 0.4629731387889662\n",
      "Cost: 0.4629720872340882\n",
      "Cost: 0.46297137529321036\n",
      "Cost: 0.4629706042495007\n",
      "Cost: 0.4629701061605547\n",
      "Cost: 0.4629698303930724\n",
      "Cost: 0.462969340547288\n",
      "Cost: 0.46296908538370896\n",
      "Cost: 0.4629686016120216\n",
      "Cost: 0.462968060780353\n",
      "Cost: 0.46296686419291144\n",
      "Cost: 0.46296649729433004\n",
      "Cost: 0.46296635308718126\n",
      "Cost: 0.46296615588806955\n",
      "Cost: 0.4629658398146133\n",
      "Cost: 0.46296498972730815\n",
      "Cost: 0.4629682126821347\n",
      "Cost: 0.46296478665390406\n",
      "Cost: 0.4629639677749485\n",
      "Cost: 0.46296396118201594\n",
      "Cost: 0.4629633137379868\n",
      "Cost: 0.46296308971282685\n",
      "Cost: 0.4629631118120573\n",
      "Cost: 0.4629629174070102\n",
      "Cost: 0.46296274883607325\n",
      "Cost: 0.4629622976428392\n",
      "Cost: 0.46296406231359455\n",
      "Cost: 0.46296219347977363\n",
      "Cost: 0.46296164632313874\n",
      "Cost: 0.4629606381304649\n",
      "Cost: 0.4629600133549223\n",
      "Cost: 0.46295936301867235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.4629590796020626\n",
      "Cost: 0.46295856824479004\n",
      "Cost: 0.46295764114789406\n",
      "Cost: 0.4629563596637556\n",
      "Cost: 0.4629556022242224\n",
      "Cost: 0.46295469764774966\n",
      "Cost: 0.4629528679037538\n",
      "Cost: 0.4629506337147238\n",
      "Cost: 0.46294885679119757\n",
      "Cost: 0.4629483301530831\n",
      "Cost: 0.4629472680729425\n",
      "Cost: 0.46294599824115373\n",
      "Cost: 0.4629428719430484\n",
      "Cost: 0.46294171003457274\n",
      "Cost: 0.46293977831622896\n",
      "Cost: 0.46293884244029404\n",
      "Cost: 0.46293807466749215\n",
      "Cost: 0.4629372336281236\n",
      "Cost: 0.46293674086801595\n",
      "Cost: 0.4629363875094262\n",
      "Cost: 0.4629352364190664\n",
      "Cost: 0.46293419439674227\n",
      "Cost: 0.4629331301395262\n",
      "Cost: 0.46293232043903493\n",
      "Cost: 0.4629320193733234\n",
      "Cost: 0.46293179342254653\n",
      "Cost: 0.46293147681601654\n",
      "Cost: 0.4629306474488206\n",
      "Cost: 0.46293032075649254\n",
      "Cost: 0.46292985087238914\n",
      "Cost: 0.46292961209351957\n",
      "Cost: 0.462929264154217\n",
      "Cost: 0.4629290718311637\n",
      "Cost: 0.46292883072863655\n",
      "Cost: 0.46292867403647864\n",
      "Cost: 0.4629284361916707\n",
      "Cost: 0.46292821639570725\n",
      "Cost: 0.46292781278843587\n",
      "Cost: 0.4629275041198555\n",
      "Cost: 0.46292667677095817\n",
      "Cost: 0.46292637288301997\n",
      "Cost: 0.46292601696740615\n",
      "Cost: 0.4629257810064304\n",
      "Cost: 0.4629251670437674\n",
      "Cost: 0.4629247231140911\n",
      "Cost: 0.4629238342903498\n",
      "Cost: 0.46292324160710807\n",
      "Cost: 0.4629218725826256\n",
      "Cost: 0.4629208774632696\n",
      "Cost: 0.4629195698930685\n",
      "Cost: 0.46291874414300627\n",
      "Cost: 0.4629166996828863\n",
      "Cost: 0.46291518689674904\n",
      "Cost: 0.4629125691688556\n",
      "Cost: 0.4629111349970744\n",
      "Cost: 0.4629071944449075\n",
      "Cost: 0.462905176100384\n",
      "Cost: 0.46290285432035233\n",
      "Cost: 0.4629016613512058\n",
      "Cost: 0.46289977482148265\n",
      "Cost: 0.46289769983096624\n",
      "Cost: 0.4628925926770866\n",
      "Cost: 0.4628897308885157\n",
      "Cost: 0.462886641057806\n",
      "Cost: 0.4628845023730609\n",
      "Cost: 0.46288129916095133\n",
      "Cost: 0.4628791593920448\n",
      "Cost: 0.46287733725480984\n",
      "Cost: 0.46287499428801293\n",
      "Cost: 0.462870116030928\n",
      "Cost: 0.4628679091882678\n",
      "Cost: 0.4628654855626907\n",
      "Cost: 0.46286302545960617\n",
      "Cost: 0.46285876336913345\n",
      "Cost: 0.46285540736473607\n",
      "Cost: 0.4628518208083344\n",
      "Cost: 0.46284959783043067\n",
      "Cost: 0.46284702159970376\n",
      "Cost: 0.46284426303994514\n",
      "Cost: 0.46284035305168103\n",
      "Cost: 0.4628379430433029\n",
      "Cost: 0.46283503692163086\n",
      "Cost: 0.46283229842919227\n",
      "Cost: 0.4628297059321377\n",
      "Cost: 0.46282710287209217\n",
      "Cost: 0.4628257810880617\n",
      "Cost: 0.46282165200236763\n",
      "Cost: 0.462819656003408\n",
      "Cost: 0.4628181814116551\n",
      "Cost: 0.46281681649499745\n",
      "Cost: 0.4628162829638399\n",
      "Cost: 0.46281522064222147\n",
      "Cost: 0.4628146952441786\n",
      "Cost: 0.4628145867567195\n",
      "Cost: 0.4628142420095307\n",
      "Cost: 0.462814006407777\n",
      "Cost: 0.462813824535085\n",
      "Cost: 0.46281362053384967\n",
      "Cost: 0.46281326649742494\n",
      "Cost: 0.46281288411482663\n",
      "Cost: 0.4628127172557798\n",
      "Cost: 0.4628133285002569\n",
      "Cost: 0.4628126758497141\n",
      "Cost: 0.46281237631030014\n",
      "Cost: 0.46281304498154585\n",
      "Cost: 0.46281226248882823\n",
      "Cost: 0.4628121745262276\n",
      "Cost: 0.4628122770240071\n",
      "Cost: 0.4628121283502685\n",
      "Cost: 0.4628120673619597\n",
      "Cost: 0.4628119197192081\n",
      "Cost: 0.46281287063888077\n",
      "Cost: 0.4628119051833536\n",
      "Cost: 0.46281172894080735\n",
      "Cost: 0.4628113796888381\n",
      "Cost: 0.462811212648518\n",
      "Cost: 0.46281092947354574\n",
      "Cost: 0.4628106956241045\n",
      "Cost: 0.462810276217933\n",
      "Cost: 0.462809808246041\n",
      "Cost: 0.46280904076986273\n",
      "Cost: 0.46280862079675555\n",
      "Cost: 0.4628081395224646\n",
      "Cost: 0.4628079043749075\n",
      "Cost: 0.4628086064494421\n",
      "Cost: 0.46280783330305963\n",
      "Cost: 0.46280806975415334\n",
      "Cost: 0.4628080458213863\n",
      "Cost: 0.4628080299361892\n",
      "Cost: 0.4628092442895881\n",
      "Cost: 0.462808078068501\n",
      "Cost: 0.4628080353554196\n",
      "Cost: 0.4628080308218664\n",
      "Cost: 0.46280803009417704\n",
      "Cost: 0.46280802996485126\n",
      "Cost: 0.4628080299414052\n",
      "Cost: 0.4628080299371388\n",
      "Cost: 0.46280802993636005\n",
      "Cost: 0.4628080299362197\n",
      "Cost: 0.4628080299361939\n",
      "Cost: 0.46280802993618864\n",
      "Cost: 0.46280802993619347\n",
      "Cost: 0.46280802993619086\n",
      "Cost: 0.4628080299361892\n",
      "Cost: 0.46280802993618897\n",
      "Cost: 0.4628080299361886\n",
      "Cost: 0.4628080299361886\n",
      "Cost: 0.46280802993618886\n",
      "Cost: 0.46280802993618864\n",
      "Cost: 0.4628080299361886\n",
      "Cost: 0.4628080299361886\n",
      "Cost: 0.4628080299361886\n",
      "Cost: 0.46280802993618864\n",
      "Cost: 0.4628080299361886\n",
      "Cost: 0.4628080458213863\n",
      "Cost: 0.46280802987288483\n",
      "Cost: 0.46280802192996495\n",
      "Cost: 0.46280810186944954\n",
      "Cost: 0.46280804011266574\n",
      "Cost: 0.4628080263175016\n",
      "Cost: 0.46280802331260806\n",
      "Cost: 0.46280802243072583\n",
      "Cost: 0.46280802212598393\n",
      "Cost: 0.46280802200950244\n",
      "Cost: 0.46280802196275495\n",
      "Cost: 0.4628080219435753\n",
      "Cost: 0.4628080219356292\n",
      "Cost: 0.4628080219323253\n",
      "Cost: 0.4628080219309493\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 0.462808\n",
      "         Iterations: 490\n",
      "         Function evaluations: 1139\n",
      "         Gradient evaluations: 1127\n",
      "Training Result: 0.46280806975415334\n",
      "Theta1 new: [[-6.15087772e-01 -2.89640377e-12  2.06212820e-12 ... -8.96785441e-05\n",
      "   1.01491337e-05 -2.47716307e-12]\n",
      " [ 3.29061805e-01 -1.34647420e-12  2.61061992e-12 ... -3.24240214e-04\n",
      "   3.86226894e-05 -1.89500975e-12]\n",
      " [-1.22925084e+00 -1.98610686e-12 -4.62172799e-13 ... -3.72739800e-05\n",
      "   2.05752139e-06  1.66183048e-12]\n",
      " ...\n",
      " [-5.54763006e-01 -2.37528215e-12 -1.96485569e-12 ... -1.91204974e-05\n",
      "   2.54292513e-06  3.13756713e-12]\n",
      " [-2.70429481e-01 -3.50394527e-12  3.48556778e-12 ...  5.77958654e-05\n",
      "  -4.08135778e-06  8.36497165e-13]\n",
      " [-2.23033717e+00  1.30235918e-12  7.51995841e-13 ... -5.07299814e-04\n",
      "   6.06341553e-05 -8.11204557e-13]]\n",
      "Theta2 new: [[-2.05834381  0.20469734  0.65512552 ...  0.67775046  0.58901703\n",
      "   0.84569252]\n",
      " [-2.81784015 -0.28286795 -1.05313633 ... -0.25360978  0.10681287\n",
      "  -1.09148147]\n",
      " [-1.67782534 -0.05293531  0.36661367 ... -0.01267975 -0.70510684\n",
      "  -0.88252953]\n",
      " ...\n",
      " [-7.41697967 -0.05867545  1.2554095  ...  0.43976636  0.7615425\n",
      "  -0.75937515]\n",
      " [-1.97819259 -0.1204776  -1.37132828 ...  0.54856459 -2.04472362\n",
      "   0.14553433]\n",
      " [-2.21206268 -0.51778456  0.11550614 ... -0.83937285 -0.84739708\n",
      "  -0.46645813]]\n",
      "Test Cost: 0.6976061752496636\n",
      "[1.42216441e-04 9.93479424e-01 2.80491567e-03 6.18437808e-04\n",
      " 1.14532389e-03 6.21705636e-03 7.74193420e-05 6.82791032e-04\n",
      " 1.59808352e-04 3.05126504e-02]\n",
      "Predicted Value is: 2\n"
     ]
    }
   ],
   "source": [
    "flattened_params = np.hstack((theta1.flatten(),theta2.flatten()))\n",
    "lambda_val = 2\n",
    "#calculate the gradients and cost\n",
    "final_res = n.nnCostFunction(flattened_params, input_layer_size, hidden_layer_size, num_labels, x, y, lambda_val)\n",
    "#Minimize:\n",
    "#flatten and merge theta1 and theta2 values into a single vector \n",
    "nn_params = flattened_params\n",
    "func_args = (input_layer_size, hidden_layer_size, num_labels, x, y, lambda_val)\n",
    "#minimize using the conjugate-gradient (cg) algorithm \n",
    "result = sp.minimize(n.nnCostFunction, x0 = nn_params, args = func_args, method = 'cg', jac = True, options = {'gtol': 0.0000001,'disp': True, 'maxiter': 1000})\n",
    "print(\"Training Result: \" + str(result.fun))\n",
    "adjusted_weights = result.x\n",
    "theta1 = np.reshape(adjusted_weights[:hidden_layer_size*(input_layer_size+1)],(hidden_layer_size, input_layer_size+1))\n",
    "theta2 = np.reshape(adjusted_weights[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "print (\"Theta1 new: \" + str(theta1))\n",
    "print(\"Theta2 new: \" + str(theta2))\n",
    "#Prediction: Training Data\n",
    "J = 0\n",
    "cost_temp = 0\n",
    "#Cost: Test Data\n",
    "for samp in range(len(x_test)):\n",
    "    x_curr = x_test[samp]\n",
    "    z2 = np.matmul(x_curr,theta1.T)\n",
    "    a2 = n.sigmoid(z2)\n",
    "    a2 = np.hstack(([1], a2))\n",
    "    z3 = np.matmul(a2,theta2.T)\n",
    "    a3 = n.sigmoid(z3)\n",
    "\n",
    "    cost_temp += n.calc_cost(y_test[samp], a3, lambda_val, len(x_test))\n",
    "\n",
    "#Accumulate cost values and regularize to get Cost(J) \n",
    "term1 = (1/len(x_test))*cost_temp\n",
    "term2 = n.regularization(1, len(x_test))\n",
    "J = term1 + term2\n",
    "print(\"Test Cost: \" + str(J))\n",
    "\n",
    "print(\"Predicted Value is: \" + str(n.predict([theta1,theta2],x[2235])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ec935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32586142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
