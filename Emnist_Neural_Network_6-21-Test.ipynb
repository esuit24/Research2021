{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "34fb3a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 6.909237746108291\n",
      "Cost: 6.909237746108291\n",
      "Cost: 3.439567230653949\n",
      "Cost: 3.711963137189134\n",
      "Cost: 3.2918454694816024\n",
      "Cost: 3.261038751494609\n",
      "Cost: 3.14324993808936\n",
      "Cost: 2.795520976787045\n",
      "Cost: 2.700780041463072\n",
      "Cost: 2.197852818155896\n",
      "Cost: 2.1198214762117003\n",
      "Cost: 2.0378288490165963\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 2.037829\n",
      "         Iterations: 5\n",
      "         Function evaluations: 11\n",
      "         Gradient evaluations: 11\n",
      "Training Result: 2.0378288490165963\n",
      "Theta1 new: [[ 0.12683818 -0.03221988 -0.00902737 ... -0.04555148  0.08173827\n",
      "  -0.06687967]\n",
      " [ 0.06814623 -0.04382481  0.10969844 ...  0.0293458   0.04472569\n",
      "   0.08013937]\n",
      " [ 0.11107106  0.10639426 -0.08310108 ...  0.00927317 -0.06101753\n",
      "  -0.00798266]\n",
      " ...\n",
      " [ 0.05413708  0.11087102 -0.08145694 ...  0.03025697  0.0663149\n",
      "   0.08674303]\n",
      " [-0.11522314  0.03222464  0.09389828 ... -0.04289645  0.10816186\n",
      "   0.02019196]\n",
      " [ 0.00812663  0.05305767 -0.09988075 ...  0.07965314  0.07348855\n",
      "  -0.10816771]]\n",
      "Theta2 new: [[ 0.02482839  0.28957488 -0.34055193 ... -0.27578336 -0.28607835\n",
      "  -0.15418691]\n",
      " [-0.17729339 -0.18550465 -0.38685855 ... -0.46463053  0.12487816\n",
      "   0.51907198]\n",
      " [-0.10487314 -0.19007224  0.23783695 ...  0.25156714  0.04199772\n",
      "   0.35325348]\n",
      " ...\n",
      " [-0.18145462 -0.09694355 -0.08949154 ...  0.19200455 -0.39242674\n",
      "  -0.14200094]\n",
      " [-0.10617809  0.24235457 -0.125249   ...  0.39247183 -0.00625038\n",
      "  -0.81073798]\n",
      " [-0.29890541 -0.41459862  0.00244016 ... -0.90588309 -0.04505842\n",
      "   0.9433493 ]]\n",
      "Test Cost: 2.0798464408800714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize as sp\n",
    "import pandas as pd\n",
    "import math\n",
    "from random import sample, uniform\n",
    "\n",
    "class Neural_Network():\n",
    "    #calculates sigmoid function, params: z (variable), returns: sigmoid calculation\n",
    "    def sigmoid(self,z):\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    #calculates the sigmoidGradient (derivative), params: z (variable), returns: sigmoid gradient calculation \n",
    "    def sigmoidGradient(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "    \n",
    "    def randomly_initialize(self, num_in, num_out):\n",
    "        epsilon = 0.12\n",
    "        size = num_out*(num_in+1)\n",
    "        all_weights = np.zeros((size,))\n",
    "        for i in range(size):   \n",
    "            all_weights[i] = uniform(-epsilon, epsilon) \n",
    "        weights = np.reshape(all_weights,(num_out,num_in+1))\n",
    "        return weights\n",
    "    \n",
    "    #calculates the regularization term for the neural network, params: lambda (regularization constant),\n",
    "    #m (number of training samples), returns regularization value\n",
    "    def regularization(self, lamda, m):\n",
    "        lamda_val = lamda/(2.0*m)\n",
    "        theta1_sum = 0 \n",
    "        theta2_sum = 0\n",
    "        for j in range(len(self.Theta1)-1):\n",
    "            for k in range(self.Theta1[0].size-1):\n",
    "                theta1_sum += self.Theta1[j+1][k+1]*self.Theta1[j+1][k+1]\n",
    "        for j in range(len(self.Theta2)-1):\n",
    "            for k in range(self.Theta2[0].size-1):\n",
    "                theta2_sum += self.Theta2[j+1][k+1]*self.Theta2[j+1][k+1]\n",
    "        return lamda_val*(theta1_sum+theta2_sum)\n",
    "    \n",
    "    #calculates the cost for the neural network, params: y_vals (expected output values), hyp (calculated output values),\n",
    "    #m (number of training samples), returns cost between given sample and expected value  \n",
    "    def calc_cost(self, y_vals, hyp, lamda, m): #hyp and y are both 10x1 vectors \n",
    "        cost = 0\n",
    "        for k in range(len(y_vals)):\n",
    "            cost += -y_vals[k] * math.log(hyp[k]) - (1-y_vals[k])*math.log(1-hyp[k])\n",
    "        return cost\n",
    "    \n",
    "    #predicts the number that correlates to the input data, params: weights(an array that consists of 2 weight matricies),\n",
    "    #x_vals (array that consists of input values), returns prediction number (0-9) \n",
    "    def predict(self, weights, x_vals):\n",
    "            #x_vals = np.hstack(([1],x_vals))\n",
    "            weights1 = weights[0]\n",
    "            weights2 = weights[1]\n",
    "            z2 = np.matmul(x_vals,weights1.T)\n",
    "            a2 = self.sigmoid(z2)\n",
    "            a2 = np.hstack(([1], a2))\n",
    "            z3 = np.matmul(a2,weights2.T)\n",
    "            a3 = self.sigmoid(z3)\n",
    "            max_val = a3[0]\n",
    "            max_index = 0\n",
    "            print(a3)\n",
    "            for i in range(len(a3)):\n",
    "                if (a3[i] > max_val):\n",
    "                    max_val = a3[i]\n",
    "                    max_index = i\n",
    "            prediction = max_index+1\n",
    "            if prediction == 10:\n",
    "                prediction = 0\n",
    "            return prediction\n",
    "        \n",
    "    #performs forward and backward prop to get a final cost value, J, and 2 gradient weight matricies\n",
    "    #params: nn_params(array that consists of 2 weight matricies for layer 1 and 2 respectively), input_layer_size (number of input units),\n",
    "    #hidden_layer_size (number of hidden units), num_labels (number of output units), x (training samples), y (expected output values), lambda_reg (regularization constant)\n",
    "    #returns cost and an array of weight gradient vectors \n",
    "    def nnCostFunction(self, nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_reg):\n",
    "        self.Theta1 = np.reshape(nn_params[:hidden_layer_size*(input_layer_size+1)],(hidden_layer_size, input_layer_size+1))\n",
    "        self.Theta2 = np.reshape(nn_params[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "        \n",
    "        J = 0;\n",
    "        Theta1_grad = np.zeros_like(self.Theta1)\n",
    "        Theta2_grad = np.zeros_like(self.Theta2)\n",
    "       \n",
    "        #Forward and Back prop: \n",
    "\n",
    "        bigDelta1 = 0\n",
    "        bigDelta2 = 0\n",
    "        cost_temp = 0\n",
    "\n",
    "        # for each training example\n",
    "        for t in range(m):\n",
    "\n",
    "            ## step 1: perform forward pass\n",
    "            x = X[t]\n",
    "\n",
    "            #calculate z2 (linear combination) and a2 (activation for layer 2)\n",
    "            z2 = np.matmul(x,self.Theta1.T)\n",
    "            a2 = self.sigmoid(z2)\n",
    "\n",
    "            # add column of ones as bias unit to the second layer\n",
    "            a2 = np.hstack(([1], a2))\n",
    "            # calculate z3 (linear combination) and a3 (activation for layer 3 aka final hypothesis)\n",
    "            z3 = np.matmul(a2,self.Theta2.T)\n",
    "            a3 = self.sigmoid(z3)\n",
    "            \n",
    "            #Backpropogation: \n",
    "\n",
    "            #step 2: set delta 3\n",
    "            delta3 = np.zeros((num_labels))\n",
    "\n",
    "            #Get Error: subtract actual val in y from each hypothesized val in a3  \n",
    "            y_vals = np.zeros((num_labels))\n",
    "            for k in range(num_labels): #for each of the 10 labels subtract\n",
    "                y_k = y[t][k]\n",
    "                y_vals[k] = y_k\n",
    "                delta3[k] = a3[k] - y_k\n",
    "\n",
    "            #step 3: for layer 2 set delta2 = Theta2 Transpose * delta3 .* sigmoidGradient(z2) (= Chain Rule)\n",
    "            #Skip over the bias unit in layer 2: no gradient calculated for this value \n",
    "            delta2 = np.matmul(self.Theta2[:,1:].T, delta3) * self.sigmoidGradient(z2)\n",
    "\n",
    "            #step 4: accumulate gradient from this sample\n",
    "            bigDelta1 += np.outer(delta2, x)\n",
    "            bigDelta2 += np.outer(delta3, a2)\n",
    "            #Update the total cost given the cost from this sample\n",
    "            cost_temp += self.calc_cost(y_vals, a3, lambda_reg, m)\n",
    "            \n",
    "        #Accumulate cost values and regularize to get Cost(J) \n",
    "        term1 = (1/m)*cost_temp\n",
    "        term2 = self.regularization(lambda_reg, m)\n",
    "        J = term1 + term2\n",
    "        print(\"Cost: \" + str(J)) \n",
    "        \n",
    "        # step 5: obtain gradient for neural net cost function by dividing the accumulated gradients by m\n",
    "        Theta1_grad = bigDelta1 / m\n",
    "        Theta2_grad = bigDelta2 / m\n",
    "        \n",
    "\n",
    "        #Regularization\n",
    "        #only regularize for j >= 1, so skip the first column\n",
    "        Theta1_grad_unregularized = np.copy(Theta1_grad)\n",
    "        Theta2_grad_unregularized = np.copy(Theta2_grad)\n",
    "        Theta1_grad += (float(lambda_reg)/m)*self.Theta1\n",
    "        Theta2_grad += (float(lambda_reg)/m)*self.Theta2\n",
    "        Theta1_grad[:,0] = Theta1_grad_unregularized[:,0]\n",
    "        Theta2_grad[:,0] = Theta2_grad_unregularized[:,0]\n",
    "        flattened_grads = np.hstack((Theta1_grad.flatten(),Theta2_grad.flatten()))\n",
    "        \n",
    "        return J, flattened_grads\n",
    "        \n",
    "        \n",
    "#Read in data files\n",
    "#df = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Theta1.csv', header = None)\n",
    "#df2 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Theta2.csv', header = None)\n",
    "df3 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/X.csv', header = None)\n",
    "df4 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Y.csv', header = None)\n",
    "#Initialize layer sizes\n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 100\n",
    "num_labels = 10\n",
    "#Set sizes for weight and data matricies\n",
    "theta1 = np.zeros([hidden_layer_size,input_layer_size+1])\n",
    "theta2 = np.zeros([num_labels,hidden_layer_size+1])\n",
    "x = np.zeros((len(df3),input_layer_size))\n",
    "x_sample = np.zeros((int(len(df3)*(0.7)), input_layer_size)) #take only 70% for training to leave 30% for testing\n",
    "y_vec = np.zeros((len(df4),))\n",
    "y_sample = np.zeros((int(len(df4)*0.7)),)\n",
    "random_indicies = sample(range(0,int(len(df3))),int(len(df3)*0.7)) \n",
    "\n",
    "n = Neural_Network()\n",
    "\n",
    "#create data and weight arrays\n",
    "theta1 = n.randomly_initialize(input_layer_size, hidden_layer_size)\n",
    "theta2 = n.randomly_initialize(hidden_layer_size, num_labels)\n",
    "\n",
    "index = 0\n",
    "while(index<len(x)):\n",
    "    x[index] = df3.iloc[index]\n",
    "    index+=1\n",
    "\n",
    "index = 0\n",
    "while (index<len(y_vec)):\n",
    "    y_vec[index] = df4.iloc[index]\n",
    "    index+=1\n",
    "    \n",
    "for index in range(len(random_indicies)):\n",
    "    sample_index = random_indicies[index] \n",
    "    x_sample[index] = x[sample_index] \n",
    "    y_sample[index] = y_vec[sample_index]\n",
    "x_test = np.zeros((int(len(df3)*0.3),input_layer_size))\n",
    "y_test = np.zeros((int(len(df4)*0.3),))\n",
    "#set test data\n",
    "test_indicies = np.zeros((int(len(df3)*0.3),))\n",
    "count = 0\n",
    "for ind in range(len(df3)):\n",
    "    if ind not in random_indicies:\n",
    "        test_indicies[count] = ind\n",
    "        count+=1\n",
    "for ii in range(len(test_indicies)):\n",
    "    test_index = int(test_indicies[ii])\n",
    "    x_test[ii] = x[test_index]\n",
    "    y_test[ii] = y_vec[test_index]\n",
    "    \n",
    "x = x_sample\n",
    "ones = np.ones((len(x_sample),1))\n",
    "test_ones = np.ones((len(x_test),1))\n",
    "x = np.hstack((ones, x)) \n",
    "x_test = np.hstack((test_ones,x_test))\n",
    "y_vec = y_sample\n",
    "    \n",
    "m = len(x)\n",
    "# set y to be a 2-D matrix with each column being a different sample and each row corresponding to a value 0-9\n",
    "y = np.zeros((m,num_labels))\n",
    "y_test_matrix = np.zeros((len(y_test),num_labels))\n",
    "# for every label, convert it into vector of 0s and a 1 in the appropriate position\n",
    "for i in range(m): #each row is new training sample\n",
    "    index = int(y_vec[i]-1)\n",
    "    y[i][index] = 1\n",
    "y_temp = y_test\n",
    "for j in range(int(len(y_test))):\n",
    "    index2 = int(y_temp[j]-1)\n",
    "    y_test_matrix[j][index2] = 1\n",
    "y_test = y_test_matrix\n",
    "nn_params = [theta1, theta2] \n",
    "\n",
    "flattened_params = np.hstack((theta1.flatten(),theta2.flatten()))\n",
    "lambda_val = 2\n",
    "#calculate the gradients and cost\n",
    "final_res = n.nnCostFunction(flattened_params, input_layer_size, hidden_layer_size, num_labels, x, y, lambda_val)\n",
    "#Minimize:\n",
    "#flatten and merge theta1 and theta2 values into a single vector \n",
    "nn_params = flattened_params\n",
    "func_args = (input_layer_size, hidden_layer_size, num_labels, x, y, lambda_val)\n",
    "#minimize using the conjugate-gradient (cg) algorithm \n",
    "result = sp.minimize(n.nnCostFunction, x0 = nn_params, args = func_args, method = 'cg', jac = True, options = {'disp': True, 'maxiter': 5})\n",
    "print(\"Training Result: \" + str(result.fun))\n",
    "adjusted_weights = result.x\n",
    "theta1 = np.reshape(adjusted_weights[:hidden_layer_size*(input_layer_size+1)],(hidden_layer_size, input_layer_size+1))\n",
    "theta2 = np.reshape(adjusted_weights[hidden_layer_size*(input_layer_size+1):], (num_labels, hidden_layer_size+1))\n",
    "print (\"Theta1 new: \" + str(theta1))\n",
    "print(\"Theta2 new: \" + str(theta2))\n",
    "#Prediction: Training Data\n",
    "J = 0\n",
    "cost_temp = 0\n",
    "#Cost: Test Data\n",
    "for samp in range(len(x_test)):\n",
    "    x_curr = x_test[samp]\n",
    "    z2 = np.matmul(x_curr,theta1.T)\n",
    "    a2 = n.sigmoid(z2)\n",
    "    a2 = np.hstack(([1], a2))\n",
    "    z3 = np.matmul(a2,theta2.T)\n",
    "    a3 = n.sigmoid(z3)\n",
    "\n",
    "    cost_temp += n.calc_cost(y_test[samp], a3, lambda_val, len(x_test))\n",
    "\n",
    "#Accumulate cost values and regularize to get Cost(J) \n",
    "term1 = (1/len(x_test))*cost_temp\n",
    "term2 = n.regularization(1, len(x_test))\n",
    "J = term1 + term2\n",
    "print(\"Test Cost: \" + str(J))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ec935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32586142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
