{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34fb3a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is: 0.3783864266414258\n",
      "Theta1 Grad:\n",
      "[[ 6.18712551e-05  0.00000000e+00  0.00000000e+00 ...  9.70111000e-09\n",
      "   2.85541444e-09  0.00000000e+00]\n",
      " [ 9.38798337e-05  0.00000000e+00  0.00000000e+00 ...  3.22774694e-08\n",
      "  -1.26324304e-10  0.00000000e+00]\n",
      " [-1.92593582e-04  0.00000000e+00  0.00000000e+00 ...  7.05404527e-08\n",
      "   1.41585607e-09  0.00000000e+00]\n",
      " ...\n",
      " [ 6.60569195e-05  0.00000000e+00  0.00000000e+00 ... -1.40472250e-08\n",
      "   1.94786063e-09  0.00000000e+00]\n",
      " [ 2.90522067e-04  0.00000000e+00  0.00000000e+00 ...  5.06149566e-07\n",
      "  -5.54722531e-08  0.00000000e+00]\n",
      " [-6.33752805e-05  0.00000000e+00  0.00000000e+00 ...  5.05494773e-09\n",
      "   4.46821988e-09  0.00000000e+00]]\n",
      "Theta2 Grad:\n",
      "[[ 6.28737627e-04  7.50946267e-04  9.87964528e-05  1.48819864e-03\n",
      "   7.31802071e-04  1.38113759e-03 -1.59325433e-04 -6.68870890e-04\n",
      "  -1.24979364e-03 -9.66226119e-05  7.19244379e-04 -5.10976190e-04\n",
      "   1.11120642e-03 -6.43551922e-04 -6.95182479e-04 -9.47091619e-04\n",
      "   2.00794716e-04  9.50724937e-04 -5.42000289e-04 -5.05540552e-05\n",
      "   2.22327564e-04  5.06964209e-04  2.45312073e-04  1.39987507e-03\n",
      "   5.07147159e-04  8.73666979e-04]\n",
      " [ 3.64629350e-04 -3.30198105e-04  7.35246429e-04 -1.10891235e-03\n",
      "   5.08782653e-04 -5.19733883e-05  1.74945574e-04 -7.39064988e-04\n",
      "   9.64388691e-04  5.79125655e-04  1.29281910e-03  1.75949394e-03\n",
      "   1.30221702e-03 -9.87783561e-04 -2.39887079e-04 -1.07227600e-03\n",
      "   1.52381720e-03  4.91374050e-04  6.97764159e-05  1.21105864e-03\n",
      "   7.54198540e-04 -6.21040566e-04 -1.76663521e-04 -1.39495589e-03\n",
      "   1.44730497e-03 -9.03758978e-04]\n",
      " [ 3.95969123e-04  1.12882466e-03 -1.25679940e-03  1.84001416e-03\n",
      "   8.15869220e-05 -9.12851835e-04 -5.68505494e-04  9.15138857e-04\n",
      "   4.64048848e-04  4.28980745e-04 -4.64979818e-04  1.51397892e-03\n",
      "  -3.32895989e-04 -3.07445964e-04 -1.49550656e-03  1.49037022e-03\n",
      "   9.94215432e-04 -6.88537262e-04  6.89197944e-04  1.12333481e-03\n",
      "   1.11253595e-03  1.94466388e-04 -1.31811584e-04  6.99115225e-04\n",
      "  -5.51270696e-04  3.21888227e-04]\n",
      " [ 4.41308845e-04 -2.49254334e-04 -3.25817436e-04  1.18603274e-04\n",
      "  -1.13145833e-03  1.44485560e-04  1.13977144e-03 -2.76071815e-04\n",
      "  -6.07058021e-04  2.00406220e-03 -9.26612072e-04 -2.01368523e-04\n",
      "   5.80461321e-04  1.58562579e-03 -5.49024611e-04  3.06042965e-04\n",
      "   4.03405279e-04  9.67974364e-04 -8.86700306e-05  1.35797208e-04\n",
      "  -1.06682993e-03 -7.54405274e-04  1.62584412e-03  6.50737785e-05\n",
      "   1.20361986e-03  8.79809313e-04]\n",
      " [ 2.52443706e-04  1.22282209e-03 -1.28856968e-03 -1.20256025e-03\n",
      "  -1.67357334e-04  1.26920837e-03  1.78409368e-04 -1.12026604e-03\n",
      "   8.98133579e-04  6.40020651e-05 -1.76857664e-04 -5.39848001e-04\n",
      "  -7.35934596e-04  1.29041364e-04  9.37325578e-04  8.67006578e-05\n",
      "   3.73720884e-04 -6.99064526e-04 -8.69607051e-04 -1.95442514e-03\n",
      "   1.30633950e-03  1.50354975e-03  1.84757592e-03  1.05434394e-03\n",
      "   4.35748743e-04 -1.55190117e-03]\n",
      " [ 5.63471529e-04 -2.32884895e-04  5.54475156e-04 -7.07707334e-05\n",
      "   2.48878478e-04 -3.26833564e-04 -2.31513102e-04  1.60280415e-03\n",
      "  -1.35952041e-03  1.27242662e-03  1.12159102e-03 -6.27548231e-04\n",
      "  -4.56304857e-04 -1.24960483e-03  6.99968717e-04  3.22403746e-04\n",
      "  -5.83872112e-04  8.74417190e-04  1.38932037e-03 -1.49304427e-04\n",
      "   2.37414840e-04  1.36928244e-03 -1.30630238e-04  7.29394593e-04\n",
      "   1.00403989e-03  5.00051325e-04]\n",
      " [ 3.59498556e-04 -6.90830805e-04  9.12420357e-04  1.21636964e-03\n",
      "   9.04321366e-04  1.46426424e-03  1.42955507e-03  8.26048275e-04\n",
      "   5.20560586e-04  4.09724769e-04 -8.70501882e-04  9.72081325e-04\n",
      "  -3.03188310e-04  4.77174147e-04 -5.26329008e-04 -1.24303344e-03\n",
      "  -2.84119874e-04 -2.25488309e-04  7.79525983e-04  3.80617226e-05\n",
      "  -8.66392989e-04  7.10746615e-04  4.60255970e-04  1.44571658e-05\n",
      "  -9.70809282e-04 -9.84907925e-04]\n",
      " [ 5.43686730e-04  4.45690557e-04  1.95268641e-03 -1.79675114e-04\n",
      "   2.22874496e-05  5.26078814e-04 -8.74793130e-04  1.37929357e-03\n",
      "   1.15031930e-03 -5.51497249e-04  9.95140615e-04 -7.29970867e-04\n",
      "  -1.23711500e-03  1.22701926e-03  1.25047264e-03  2.12639232e-04\n",
      "   1.39420852e-03  1.80320414e-03 -4.51487821e-04  1.64887913e-03\n",
      "   7.71735890e-04 -1.09614374e-03  7.32915873e-04  2.57511487e-04\n",
      "  -3.75811467e-04  1.27957446e-03]\n",
      " [ 8.49655307e-04 -6.88848349e-05 -5.61678055e-04  7.82060250e-04\n",
      "   2.59685236e-03 -1.42445372e-05  1.76594323e-03 -2.83670761e-04\n",
      "   7.23534345e-04 -1.12572228e-03  1.30840815e-04 -5.66125448e-05\n",
      "   9.68861608e-04  5.24539427e-04  1.33314110e-03  1.82862706e-03\n",
      "  -1.21907564e-04  7.75819210e-04 -2.19923826e-04 -2.26989191e-04\n",
      "   1.31976021e-03 -3.15112109e-05 -7.74878079e-04  5.00631385e-04\n",
      "   1.13453369e-03  1.35707295e-03]\n",
      " [ 2.47554499e-04  8.40622498e-04  7.49803374e-04 -4.75889813e-04\n",
      "  -2.75108887e-04  1.63093186e-04 -3.86608062e-04  4.07194376e-04\n",
      "   4.14276470e-04 -6.44816945e-05  7.60309756e-04  1.06863697e-03\n",
      "   1.63130504e-03  1.30124973e-03  1.02705961e-03  5.40580974e-04\n",
      "  -5.61650699e-04 -7.20436371e-04  1.13423425e-03 -2.88384085e-04\n",
      "  -1.06346989e-03  8.28191171e-04 -5.38570825e-04  9.66104693e-05\n",
      "  -7.57736847e-04  7.73329872e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "class Neural_Network():\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "    def sigmoidGradient(self,z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "    \n",
    "    def regularization(self, lamda, m):\n",
    "        lamda_val = lamda/(2.0*m)\n",
    "        theta1_sum = 0 \n",
    "        theta2_sum = 0\n",
    "        for j in range(len(self.Theta1)-1):\n",
    "            for k in range(self.Theta1[0].size-1):\n",
    "                theta1_sum += self.Theta1[j+1][k+1]*self.Theta1[j+1][k+1]\n",
    "        for j in range(len(self.Theta2)-1):\n",
    "            for k in range(self.Theta2[0].size-1):\n",
    "                theta2_sum += self.Theta2[j+1][k+1]*self.Theta2[j+1][k+1]\n",
    "        return lamda_val*(theta1_sum+theta2_sum)\n",
    "    \n",
    "    def calc_cost(self, y_vals, hyp, lamda, m): #hyp and y are both 10x1 vectors \n",
    "        cost = 0\n",
    "        for k in range(y_vals.size):\n",
    "            cost += -y_vals[k] * math.log(hyp[k]) - (1-y_vals[k])*math.log(1-hyp[k])\n",
    "        return cost\n",
    "    \n",
    "    def nnCostFunction(self, nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_reg):\n",
    "\n",
    "        #Theta1 = np.reshape(nn_params[:hidden_layer_size * (input_layer_size + 1)], (hidden_layer_size, input_layer_size + 1), order='F')\n",
    "        self.Theta1 = nn_params[0]\n",
    "        #Theta2 = np.reshape(nn_params[hidden_layer_size * (input_layer_size + 1):], (num_labels, hidden_layer_size + 1), order='F')\n",
    "        self.Theta2 = nn_params[1]\n",
    "        m = len(X)\n",
    "        labels = y.flatten()\n",
    "        # set y to be matrix of size m x k\n",
    "        y = np.zeros((m,num_labels))\n",
    "        # for every label, convert it into vector of 0s and a 1 in the appropriate position\n",
    "        for i in range(m): #each row is new training sample\n",
    "            index = int(labels[i]-1)\n",
    "            y[i][index] = 1\n",
    "        J = 0;\n",
    "        Theta1_grad = np.zeros_like(self.Theta1)\n",
    "        Theta2_grad = np.zeros_like(self.Theta2)\n",
    "\n",
    "        # add column of ones as bias unit from input layer to second layer\n",
    "        X = np.hstack((np.ones((m,1)), X)) # = a1\n",
    "        \n",
    "        #Forward and Back prop: \n",
    "\n",
    "        bigDelta1 = 0\n",
    "        bigDelta2 = 0\n",
    "        cost_temp = 0\n",
    "\n",
    "        # for each training example\n",
    "        for t in range(m):\n",
    "\n",
    "\n",
    "            ## step 1: perform forward pass\n",
    "            # set lowercase x to the t-th row of X\n",
    "            x = X[t]\n",
    "            # note that uppercase X already included column of ones \n",
    "            # as bias unit from input layer to second layer, so no need to add it\n",
    "\n",
    "            # calculate second layer as sigmoid( z2 ) where z2 = Theta1 * a1\n",
    "            z2 = np.matmul(x,self.Theta1.T)\n",
    "            a2 = self.sigmoid(z2)\n",
    "\n",
    "            # add column of ones as bias unit from second layer to third layer\n",
    "            a2 = np.concatenate((np.array([1]), a2))\n",
    "            # calculate third layer as sigmoid (z3) where z3 = Theta2 * a2\n",
    "            z3 = np.matmul(a2,self.Theta2.T)\n",
    "            a3 = self.sigmoid(z3)\n",
    "\n",
    "            ## step 2: \n",
    "            delta3 = np.zeros((num_labels))\n",
    "\n",
    "            #subtract actual val in y from each hypothesized val in a3\n",
    "            y_vals = np.zeros((num_labels))\n",
    "            for k in range(num_labels): #for each of the 10 labels subtract\n",
    "                y_k = y[t][k]\n",
    "                y_vals[k] = y_k\n",
    "                delta3[k] = a3[k] - y_k\\\n",
    "\n",
    "            ## step 3: for the hidden layer l=2, set delta2 = Theta2' * delta3 .* sigmoidGradient(z2)\n",
    "            # note that we're skipping delta2_0 (=gradients of bias units, which we don't use here)\n",
    "            delta2 = np.matmul(self.Theta2[:,1:].T, delta3) * self.sigmoidGradient(z2)\n",
    "\n",
    "            ## step 4: accumulate gradient from this example\n",
    "            # accumulation\n",
    "            bigDelta1 += np.outer(delta2, x)\n",
    "            bigDelta2 += np.outer(delta3, a2)\n",
    "\n",
    "            cost_temp += self.calc_cost(y_vals, a3, lambda_reg, m)\n",
    "        term1 = (1/m)*cost_temp\n",
    "        term2 = self.regularization(lambda_reg, m)\n",
    "        J = term1 + term2\n",
    "        print(\"Cost is: \" + str(J))\n",
    "        # step 5: obtain gradient for neural net cost function by dividing the accumulated gradients by m\n",
    "        Theta1_grad = bigDelta1 / m\n",
    "        Theta2_grad = bigDelta2 / m\n",
    "        print(\"Theta1 Grad:\")\n",
    "        print(Theta1_grad)\n",
    "        print(\"Theta2 Grad:\")\n",
    "        print(Theta2_grad)\n",
    "        \n",
    "\n",
    "        #% REGULARIZATION FOR GRADIENT\n",
    "        # only regularize for j >= 1, so skip the first column\n",
    "        Theta1_grad_unregularized = np.copy(Theta1_grad)\n",
    "        Theta2_grad_unregularized = np.copy(Theta2_grad)\n",
    "        Theta1_grad += (float(lambda_reg)/m)*self.Theta1\n",
    "        Theta2_grad += (float(lambda_reg)/m)*self.Theta2\n",
    "        Theta1_grad[:,0] = Theta1_grad_unregularized[:,0]\n",
    "        Theta2_grad[:,0] = Theta2_grad_unregularized[:,0]\n",
    "        \n",
    "\n",
    "df = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Theta1.csv', header = None)\n",
    "df2 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Theta2.csv', header = None)\n",
    "df3 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/X.csv', header = None)\n",
    "df4 = pd.read_csv(r'/Users/elliesuit/emnist/Emnist Data/Y.csv', header = None)\n",
    "theta1 = np.zeros([25,401])\n",
    "theta2 = np.zeros([10,26])\n",
    "x = np.zeros([5000,400])\n",
    "y = np.zeros([5000,1])\n",
    "index = 0 \n",
    "while (index < 25):\n",
    "    theta1[index] = df.iloc[index]\n",
    "    index+=1\n",
    "\n",
    "index = 0\n",
    "while(index<10):\n",
    "    theta2[index] = df2.iloc[index]\n",
    "    index+=1\n",
    "\n",
    "index = 0\n",
    "while(index<5000):\n",
    "    x[index] = df3.iloc[index]\n",
    "    index+=1\n",
    "ones = np.ones((5000,1))\n",
    "#x = np.hstack((ones, Xtemp)) #this is a1 (67) \n",
    "\n",
    "index = 0\n",
    "while (index<5000):\n",
    "    y[index] = df4.iloc[index]\n",
    "    index+=1\n",
    "\n",
    "nn_params = [theta1, theta2] \n",
    "input_layer_size = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "lambda_val = 1 \n",
    "n = Neural_Network()\n",
    "n.nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, x, y, lambda_val)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ec935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32586142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
